{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if dataset loads\n",
    "import csv\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import keras\n",
    "\n",
    "dataset_path = \"../../data/speed_challenge_2017/data/train_seq/\"\n",
    "output_path = \"../../data/speed_challenge_2017/data/train.txt\"\n",
    "\n",
    "image_paths = sorted(os.listdir(dataset_path))\n",
    "\n",
    "with open(output_path, 'r') as h:\n",
    "    lines = h.readlines()\n",
    "\n",
    "dataset = []\n",
    "for i in range(len(image_paths)):\n",
    "    speed_val = float(lines[i].strip())\n",
    "    dataset.append({\"image\": image_paths[i], \"speed\": speed_val/15.0 - 1.0})\n",
    "\n",
    "print(len(dataset))\n",
    "\n",
    "# dataset = [dataset[i] for i in range(len(dataset)) if i%2 == 0]\n",
    "\n",
    "# print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ToDO\n",
    "# Better training of RNN by reducing the frequency\n",
    "# Input type, maybe saturation\n",
    "# Providing additional features\n",
    "\n",
    "# Sample processing\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img = cv2.imread(dataset_path + dataset[5]['image'])[30:370, :]\n",
    "plt.imshow(img)\n",
    "plt.show()\n",
    "\n",
    "img_b = cv2.imread(dataset_path + dataset[5]['image'], 0)\n",
    "plt.imshow(img_b)\n",
    "plt.show()\n",
    "\n",
    "# sat = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:, :, 1]\n",
    "# plt.imshow(sat)\n",
    "# plt.show()\n",
    "\n",
    "# Equalize hist\n",
    "equ = cv2.equalizeHist(img_b)\n",
    "print(equ.shape)\n",
    "#res = np.hstack((img_b, sat, equ))\n",
    "plt.imshow(equ)\n",
    "plt.show()\n",
    "\n",
    "# for i in range(len(dataset)):\n",
    "#     img = cv2.imread(dataset[i]['image'])[30:340, :, :]\n",
    "#     img_b = cv2.imread(dataset[i]['image'], 0)[30:340, :]\n",
    "#     sat = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:, :, 2]\n",
    "#     equ = cv2.equalizeHist(img_b)\n",
    "#     res = np.vstack((np.hstack((img_b, equ)), np.hstack((img_b, sat))))\n",
    "#     cv2.imshow(\"view\", res)\n",
    "#     if chr(cv2.waitKey(0)) == 'q':\n",
    "#         break\n",
    "\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "cols = 640\n",
    "rows = 340\n",
    "\n",
    "seq_len = 10\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "train_set, valid_set = train_test_split(dataset, test_size=0.10)\n",
    "\n",
    "print(\"train set has {} elements\".format(len(train_set)))\n",
    "print(\"valid set has {} elements\".format(len(valid_set)))\n",
    "\n",
    "def data_generator(path, dataset, seq_len):\n",
    "    batch_seq_images = np.zeros((BATCH_SIZE, seq_len, rows, cols, 3))\n",
    "    batch_seq_speed = np.zeros((BATCH_SIZE, seq_len, 1))\n",
    "\n",
    "    while 1:\n",
    "        for i in range(BATCH_SIZE):\n",
    "            while 1:\n",
    "                index = np.random.randint(len(dataset))\n",
    "                if index + seq_len <= len(dataset):\n",
    "                    seq_speed = []\n",
    "                    seq_images = []\n",
    "                    for j in range(index, index + seq_len):\n",
    "                        seq_speed.append(dataset[j][\"speed\"])\n",
    "                        img = cv2.imread(dataset_path + dataset[j][\"image\"])[30:370, :, :]\n",
    "                        #img = cv2.equalizeHist(img)\n",
    "                        img = np.asarray(img).reshape(rows, cols, 3)\n",
    "                        seq_images.append(img)\n",
    "                    seq_speed = np.array(seq_speed)\n",
    "                    seq_images = np.array(seq_images)\n",
    "                    break\n",
    "                else:\n",
    "                    continue\n",
    "            batch_seq_images[i] = seq_images\n",
    "            batch_seq_speed[i] = seq_speed.reshape(seq_len, 1)\n",
    "\n",
    "        # for ru\n",
    "        yield batch_seq_images, batch_seq_speed[:, -1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers.core import Lambda, Dense, Activation, Flatten, Dropout\n",
    "from keras.layers.convolutional import Cropping2D, Convolution2D, MaxPooling2D, AveragePooling2D\n",
    "from keras.layers.advanced_activations import ELU\n",
    "from keras.layers.recurrent import GRU, LSTM\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.applications.vgg16 import VGG16\n",
    "\n",
    "print(\"\\nBuilding and compiling the model ...\")\n",
    "\n",
    "vgg = VGG16(include_top=False, weights='imagenet', input_tensor=None, \n",
    "              input_shape=(rows, cols, 3), classes=1000)\n",
    "\n",
    "layer_dict = dict([(layer.name, layer) for layer in vgg.layers])\n",
    "\n",
    "model = Sequential()\n",
    "model.add(TimeDistributed(Lambda(lambda x: (x / 127.5) - 1.0), input_shape=(None, rows, cols, 3)))\n",
    "model.add(TimeDistributed(Convolution2D(64, 3, 3, activation='relu', weights=vgg.layers[1].get_weights()), \n",
    "                                        name='block1_conv1'))\n",
    "model.add(TimeDistributed(Convolution2D(64, 3, 3, weights=vgg.layers[2].get_weights(), activation='relu'), name='block1_conv2'))\n",
    "model.add(TimeDistributed(MaxPooling2D((2, 2), strides=(2, 2))))\n",
    "\n",
    "model.add(TimeDistributed(Convolution2D(128, 3, 3, activation='relu', weights=vgg.layers[4].get_weights()), name='block2_conv1'))\n",
    "model.add(TimeDistributed(Convolution2D(128, 3, 3, activation='relu', weights=vgg.layers[5].get_weights()), name='block2_conv2'))\n",
    "model.add(TimeDistributed(MaxPooling2D((2, 2), strides=(2, 2))))\n",
    "\n",
    "model.add(TimeDistributed(Convolution2D(256, 3, 3, activation='relu', weights=vgg.layers[7].get_weights()), name='block3_conv1'))\n",
    "model.add(TimeDistributed(Convolution2D(256, 3, 3, activation='relu', weights=vgg.layers[8].get_weights()), name='block3_conv2'))\n",
    "model.add(TimeDistributed(Convolution2D(256, 3, 3, activation='relu', weights=vgg.layers[9].get_weights()), name='block3_conv3'))\n",
    "model.add(TimeDistributed(MaxPooling2D((2, 2), strides=(2, 2))))\n",
    "\n",
    "model.add(TimeDistributed(Convolution2D(512, 3, 3, activation='relu', weights=vgg.layers[11].get_weights()), name='block4_conv1'))\n",
    "model.add(TimeDistributed(Convolution2D(512, 3, 3, activation='relu', weights=vgg.layers[12].get_weights()), name='block4_conv2'))\n",
    "model.add(TimeDistributed(Convolution2D(512, 3, 3, activation='relu', weights=vgg.layers[13].get_weights()), name='block4_conv3'))\n",
    "model.add(TimeDistributed(MaxPooling2D((2, 2), strides=(2, 2))))\n",
    "\n",
    "model.add(TimeDistributed(Convolution2D(512, 3, 3, activation='relu', weights=vgg.layers[15].get_weights()), name='block5_conv1'))\n",
    "model.add(TimeDistributed(Convolution2D(512, 3, 3, activation='relu', weights=vgg.layers[16].get_weights()), name='block5_conv2'))\n",
    "model.add(TimeDistributed(Convolution2D(512, 3, 3, activation='relu', weights=vgg.layers[17].get_weights()), name='block5_conv3'))\n",
    "model.add(TimeDistributed(MaxPooling2D((2, 2), strides=(2, 2))))\n",
    "model.add(TimeDistributed(Flatten(name='flat')))\n",
    "model.add(LSTM(16, return_sequences=False, init='glorot_uniform', inner_init='glorot_uniform', activation='relu',\n",
    "             name='GRU1'))\n",
    "model.add(Dense(1, name='output')) \n",
    "\n",
    "for layer in model.layers[:11]:\n",
    "    layer.trainable = False\n",
    "\n",
    "vgg.summary()\n",
    "model.summary()\n",
    "\n",
    "adam = Adam()\n",
    "#rmsprop = RMSprop()\n",
    "model.compile(loss='mean_squared_error', optimizer=adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.callbacks import Callback\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import math\n",
    "\n",
    "class LifeCycleCallBack(keras.callbacks.Callback):\n",
    "    \n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        pass\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        pass\n",
    "\n",
    "    def on_batch_begin(self, batch, logs={}):\n",
    "        pass\n",
    "    \n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        \n",
    "    def on_train_begin(self, logs={}):\n",
    "        print('Beginning training')\n",
    "        self.losses = []\n",
    "        \n",
    "    def on_train_end(self, logs={}):\n",
    "        print('Ending Training')\n",
    "\n",
    "\n",
    "NUM_EPOCHS = 250\n",
    "        \n",
    "lifecycle_callback = LifeCycleCallBack()\n",
    "\n",
    "tensorboard = keras.callbacks.TensorBoard(log_dir='../../data/logs/speed_challenge/', histogram_freq=1, \n",
    "                                          write_graph=True, write_images=True)\n",
    "\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(\"../../data/weights/speed_challenge/model.h5\", monitor='val_loss', verbose=0, \n",
    "                                             save_best_only=True, save_weights_only=True, mode='auto', period=1)\n",
    "\n",
    "train_generator = data_generator(dataset_path, train_set, seq_len)\n",
    "valid_generator = data_generator(dataset_path, valid_set, seq_len)\n",
    "\n",
    "samples_per_epoch = math.ceil((len(train_set) - seq_len)/(seq_len*BATCH_SIZE))*BATCH_SIZE\n",
    "nb_val_samples = math.ceil((len(valid_set) - seq_len)/(seq_len*BATCH_SIZE))*BATCH_SIZE\n",
    "\n",
    "print(\"\\nSaving Model...\")\n",
    "\n",
    "model_json = model.to_json()\n",
    "with open(\"../../data/weights/speed_challenge/model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "#model.save_weights(\"model.h5\")\n",
    "print(\"Model Saved.\")\n",
    "\n",
    "\n",
    "history = model.fit_generator(train_generator,\n",
    "                              validation_data=valid_generator,\n",
    "                              samples_per_epoch=samples_per_epoch,\n",
    "                              nb_val_samples=nb_val_samples,\n",
    "                              nb_epoch=NUM_EPOCHS, verbose=1, \n",
    "                              callbacks=[lifecycle_callback, checkpoint])\n",
    "\n",
    "print(\"\\nTraining the model ended.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
