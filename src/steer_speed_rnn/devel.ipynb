{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 45999 samples from file ../../data/udacity_sim_data/\n",
      "Loaded 1328 samples from file ../../data/track2/seq1/\n"
     ]
    }
   ],
   "source": [
    "#Check if dataset loads\n",
    "import csv\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import keras\n",
    "\n",
    "DATASET_PATH = \"../../data/udacity_sim_data/\"\n",
    "VAL_PATH = \"../../data/track2/seq1/\"\n",
    "def load_dataset(file_path):\n",
    "    '''\n",
    "    Loads dataset in memory\n",
    "    '''\n",
    "    dataset = []\n",
    "    with open(file_path) as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        for line in reader:\n",
    "            try:\n",
    "                dataset.append({'center':line[0], 'left':line[1], 'right':line[2], 'steering':float(line[3]), \n",
    "                            'throttle':float(line[4]), 'brake':float(line[5]), 'speed':float(line[6])/15.0 - 1})\n",
    "            except:\n",
    "                continue\n",
    "    return dataset\n",
    "\n",
    "dataset = load_dataset(os.path.join(DATASET_PATH, \"driving_log.csv\"))\n",
    "val_dataset = load_dataset(os.path.join(VAL_PATH, \"driving_log.csv\"))\n",
    "\n",
    "print(\"Loaded {} samples from file {}\".format(len(dataset),DATASET_PATH))\n",
    "print(\"Loaded {} samples from file {}\".format(len(val_dataset),VAL_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set has 45999 elements\n",
      "valid set has 1328 elements\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "sample_file = os.path.join(DATASET_PATH, dataset[0]['center'].strip())\n",
    "INPUT_IMAGE_ROWS, INPUT_IMAGE_COLS, INPUT_IMAGE_CHANNELS = 160, 320, 1\n",
    "\n",
    "seq_len = 10\n",
    "BATCH_SIZE = 10\n",
    "\n",
    "train_set = dataset\n",
    "valid_set = val_dataset\n",
    "\n",
    "print(\"train set has {} elements\".format(len(train_set)))\n",
    "print(\"valid set has {} elements\".format(len(valid_set)))\n",
    "\n",
    "def data_generator(path, dataset, seq_len):\n",
    "    batch_seq_images = np.zeros((BATCH_SIZE, seq_len, 160, 320, 1))\n",
    "    batch_seq_speed = np.zeros((BATCH_SIZE, seq_len, 1))\n",
    "    \n",
    "    while 1:\n",
    "        for i in range(BATCH_SIZE):\n",
    "            while 1:\n",
    "                index = np.random.randint(len(dataset))\n",
    "                if index + seq_len <= len(dataset):\n",
    "                    seq_speed = []\n",
    "                    seq_images = []\n",
    "                    for j in range(index, index + seq_len):\n",
    "                        seq_speed.append(dataset[j][\"speed\"])\n",
    "                        img = cv2.imread(path + dataset[j][\"center\"].strip())\n",
    "                        img = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)[:, :, 1]\n",
    "                        img = np.asarray(img).reshape(160, 320, 1)\n",
    "                        seq_images.append(img)\n",
    "                    seq_speed = np.array(seq_speed)\n",
    "                    seq_images = np.array(seq_images)\n",
    "                    break\n",
    "                else:\n",
    "                    continue\n",
    "            batch_seq_images[i] = seq_images\n",
    "            batch_seq_speed[i] = seq_speed.reshape(seq_len, 1)\n",
    "            #batch_seq_steering_angles[i] = seq_steering_angles\n",
    "\n",
    "        # for ru\n",
    "        yield batch_seq_images, batch_seq_speed[:, -1, :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.74089933]\n",
      " [ 0.24607   ]\n",
      " [ 1.002306  ]\n",
      " [ 0.26081867]\n",
      " [ 0.57009267]\n",
      " [ 1.00216133]\n",
      " [ 1.00148733]\n",
      " [-0.38371633]\n",
      " [ 0.45018867]\n",
      " [ 0.11120867]]\n"
     ]
    }
   ],
   "source": [
    "sample_gen = data_generator(DATASET_PATH, train_set, seq_len)\n",
    "x, y = next(sample_gen)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building and compiling the model ...\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "timedistributed_11 (TimeDistribu (None, None, 160, 320 0           timedistributed_input_3[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "timedistributed_12 (TimeDistribu (None, None, 75, 155, 122         timedistributed_11[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "timedistributed_13 (TimeDistribu (None, None, 36, 76,  26          timedistributed_12[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "timedistributed_14 (TimeDistribu (None, None, 2736)    0           timedistributed_13[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "activation_3 (Activation)        (None, None, 2736)    0           timedistributed_14[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "GRU1 (GRU)                       (None, 16)            132144      activation_3[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)              (None, 16)            0           GRU1[0][0]                       \n",
      "____________________________________________________________________________________________________\n",
      "output (Dense)                   (None, 1)             17          dropout_4[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 132,309\n",
      "Trainable params: 132,309\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers.core import Lambda, Dense, Activation, Flatten, Dropout\n",
    "from keras.layers.convolutional import Cropping2D, Convolution2D, MaxPooling2D\n",
    "from keras.layers.advanced_activations import ELU\n",
    "from keras.layers.recurrent import GRU, LSTM\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "\n",
    "print(\"\\nBuilding and compiling the model ...\")\n",
    "\n",
    "model = Sequential()\n",
    "# Preprocess incoming data, centered around zero with small standard deviation \n",
    "model.add(TimeDistributed(Lambda(lambda x: (x / 127.5) - 1.0), input_shape=(None, INPUT_IMAGE_ROWS, INPUT_IMAGE_COLS, INPUT_IMAGE_CHANNELS)))\n",
    "# Block - conv\n",
    "model.add(TimeDistributed(Convolution2D(1, 11, 11, border_mode='valid', subsample=[2, 2], init='glorot_uniform', activation='relu', name='Conv1')))\n",
    "# Block - conv\n",
    "model.add(TimeDistributed(Convolution2D(1, 5, 5, border_mode='valid', subsample=[2, 2], init='glorot_uniform', activation='relu', name='Conv2')))\n",
    "# Block - conv\n",
    "#model.add(Convolution2D(1, 3, 3, border_mode='valid', subsample=[2, 2], init='glorot_uniform', activation='relu', name='Conv3'))\n",
    "# Block - conv\n",
    "#model.add(Convolution2D(1, 3, 3, border_mode='valid', init='glorot_uniform', activation='relu', name='Conv4'))\n",
    "# Block - flatten\n",
    "#model.add(TimeDistributed(MaxPooling2D((4,4),(4,4),'valid', name='pool1')))\n",
    "model.add(TimeDistributed(Flatten(name='flat')))\n",
    "model.add(Activation('relu'))\n",
    "#model.add(TimeDistributed(Dropout(0.20)))\n",
    "model.add(GRU(16, return_sequences=False, init='glorot_uniform', inner_init='glorot_uniform', activation='relu',\n",
    "             name='GRU1'))\n",
    "model.add(Dropout(0.20))\n",
    "model.add(Dense(1, name='output')) \n",
    "model.summary()\n",
    "\n",
    "#adam = Adam(lr=0.001)\n",
    "rmsprop = RMSprop()\n",
    "model.compile(loss='mean_squared_error', optimizer=rmsprop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving Model...\n",
      "Model Saved.\n",
      "Beginning training\n",
      "Epoch 1/250\n",
      "4600/4600 [==============================] - 54s - loss: 0.2206 - val_loss: 0.1582\n",
      "Epoch 2/250\n",
      "4600/4600 [==============================] - 53s - loss: 0.1622 - val_loss: 0.2096\n",
      "Epoch 3/250\n",
      "4600/4600 [==============================] - 53s - loss: 0.1454 - val_loss: 0.2170\n",
      "Epoch 4/250\n",
      "4600/4600 [==============================] - 53s - loss: 0.1300 - val_loss: 0.1604\n",
      "Epoch 5/250\n",
      "4600/4600 [==============================] - 53s - loss: 0.1247 - val_loss: 0.1681\n",
      "Epoch 6/250\n",
      "4600/4600 [==============================] - 53s - loss: 0.1087 - val_loss: 0.1672\n",
      "Epoch 7/250\n",
      "4600/4600 [==============================] - 53s - loss: 0.0975 - val_loss: 0.1212\n",
      "Epoch 8/250\n",
      "4600/4600 [==============================] - 53s - loss: 0.0872 - val_loss: 0.1895\n",
      "Epoch 9/250\n",
      "4600/4600 [==============================] - 53s - loss: 0.0836 - val_loss: 0.1657\n",
      "Epoch 10/250\n",
      "4600/4600 [==============================] - 53s - loss: 0.0770 - val_loss: 0.1632\n",
      "Epoch 11/250\n",
      "4600/4600 [==============================] - 53s - loss: 0.0725 - val_loss: 0.1147\n",
      "Epoch 12/250\n",
      "4600/4600 [==============================] - 53s - loss: 0.0714 - val_loss: 0.1339\n",
      "Epoch 13/250\n",
      "4600/4600 [==============================] - 53s - loss: 0.0708 - val_loss: 0.1615\n",
      "Epoch 14/250\n",
      "4600/4600 [==============================] - 53s - loss: 0.0638 - val_loss: 0.1285\n",
      "Epoch 15/250\n",
      "4600/4600 [==============================] - 53s - loss: 0.0600 - val_loss: 0.1701\n",
      "Epoch 16/250\n",
      "4600/4600 [==============================] - 53s - loss: 0.0616 - val_loss: 0.1415\n",
      "Epoch 17/250\n",
      "4600/4600 [==============================] - 53s - loss: 0.0539 - val_loss: 0.1431\n",
      "Epoch 18/250\n",
      "4600/4600 [==============================] - 53s - loss: 0.0490 - val_loss: 0.1941\n",
      "Epoch 19/250\n",
      "4600/4600 [==============================] - 53s - loss: 0.0509 - val_loss: 0.1744\n",
      "Epoch 20/250\n",
      "4600/4600 [==============================] - 53s - loss: 0.0489 - val_loss: 0.1310\n",
      "Epoch 21/250\n",
      "4600/4600 [==============================] - 53s - loss: 0.0520 - val_loss: 0.1157\n",
      "Epoch 22/250\n",
      "4600/4600 [==============================] - 53s - loss: 0.0480 - val_loss: 0.1289\n",
      "Epoch 23/250\n",
      "4600/4600 [==============================] - 53s - loss: 0.0468 - val_loss: 0.1664\n",
      "Epoch 24/250\n",
      "4600/4600 [==============================] - 53s - loss: 0.0432 - val_loss: 0.1355\n",
      "Epoch 25/250\n",
      "4600/4600 [==============================] - 53s - loss: 0.0455 - val_loss: 0.1755\n",
      "Epoch 26/250\n",
      "4600/4600 [==============================] - 53s - loss: 0.0424 - val_loss: 0.1743\n",
      "Epoch 27/250\n",
      "4600/4600 [==============================] - 53s - loss: 0.0453 - val_loss: 0.1367\n",
      "Epoch 28/250\n",
      "4600/4600 [==============================] - 53s - loss: 0.0412 - val_loss: 0.1673\n",
      "Epoch 29/250\n",
      "4600/4600 [==============================] - 53s - loss: 0.0399 - val_loss: 0.2026\n",
      "Epoch 30/250\n",
      "4600/4600 [==============================] - 53s - loss: 0.0405 - val_loss: 0.1986\n",
      "Epoch 31/250\n",
      "4600/4600 [==============================] - 53s - loss: 0.0402 - val_loss: 0.1540\n",
      "Epoch 32/250\n",
      "4600/4600 [==============================] - 53s - loss: 0.0413 - val_loss: 0.1117\n",
      "Epoch 33/250\n",
      "4600/4600 [==============================] - 53s - loss: 0.0391 - val_loss: 0.1321\n",
      "Epoch 34/250\n",
      "4600/4600 [==============================] - 53s - loss: 0.0380 - val_loss: 0.1519\n",
      "Epoch 35/250\n",
      "4600/4600 [==============================] - 53s - loss: 0.0386 - val_loss: 0.1707\n",
      "Epoch 36/250\n",
      "4600/4600 [==============================] - 53s - loss: 0.0385 - val_loss: 0.1965\n",
      "Epoch 37/250\n",
      "4600/4600 [==============================] - 53s - loss: 0.0361 - val_loss: 0.1657\n",
      "Epoch 38/250\n",
      "4600/4600 [==============================] - 53s - loss: 0.0359 - val_loss: 0.1587\n",
      "Epoch 39/250\n",
      "4600/4600 [==============================] - 53s - loss: 0.0382 - val_loss: 0.1731\n",
      "Epoch 40/250\n",
      "4600/4600 [==============================] - 53s - loss: 0.0370 - val_loss: 0.1934\n",
      "Epoch 41/250\n",
      "4600/4600 [==============================] - 53s - loss: 0.0372 - val_loss: 0.1530\n",
      "Epoch 42/250\n",
      "4600/4600 [==============================] - 53s - loss: 0.0355 - val_loss: 0.1576\n",
      "Epoch 43/250\n",
      "4600/4600 [==============================] - 53s - loss: 0.0347 - val_loss: 0.1544\n",
      "Epoch 44/250\n",
      "4600/4600 [==============================] - 53s - loss: 0.0349 - val_loss: 0.1528\n",
      "Epoch 45/250\n",
      "2590/4600 [===============>..............] - ETA: 22s - loss: 0.0319"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-a0cd9f99f771>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m                               \u001b[0mnb_val_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnb_val_samples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m                               \u001b[0mnb_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m                               callbacks=[lifecycle_callback, checkpoint])\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nTraining the model ended.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/udacity/lib/python3.5/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, samples_per_epoch, nb_epoch, verbose, callbacks, validation_data, nb_val_samples, class_weight, max_q_size, nb_worker, pickle_safe, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    933\u001b[0m                                         \u001b[0mnb_worker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnb_worker\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    934\u001b[0m                                         \u001b[0mpickle_safe\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpickle_safe\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 935\u001b[0;31m                                         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    936\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    937\u001b[0m     def evaluate_generator(self, generator, val_samples,\n",
      "\u001b[0;32m~/anaconda/envs/udacity/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, samples_per_epoch, nb_epoch, verbose, callbacks, validation_data, nb_val_samples, class_weight, max_q_size, nb_worker, pickle_safe, initial_epoch)\u001b[0m\n\u001b[1;32m   1520\u001b[0m                             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1521\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1522\u001b[0;31m                             \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1523\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1524\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__len__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.callbacks import Callback\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import math\n",
    "\n",
    "class LifeCycleCallBack(keras.callbacks.Callback):\n",
    "    \n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        pass\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        pass\n",
    "\n",
    "    def on_batch_begin(self, batch, logs={}):\n",
    "        pass\n",
    "    \n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        \n",
    "    def on_train_begin(self, logs={}):\n",
    "        print('Beginning training')\n",
    "        self.losses = []\n",
    "        \n",
    "    def on_train_end(self, logs={}):\n",
    "        print('Ending Training')\n",
    "\n",
    "\n",
    "NUM_EPOCHS = 250\n",
    "        \n",
    "lifecycle_callback = LifeCycleCallBack()\n",
    "\n",
    "tensorboard = keras.callbacks.TensorBoard(log_dir='../../data/logs/speed_cnn_rnn/', histogram_freq=1, \n",
    "                                          write_graph=True, write_images=True)\n",
    "\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(\"../../data/weights/speed_cnn_rnn/model.h5\", monitor='val_loss', verbose=0, \n",
    "                                             save_best_only=True, save_weights_only=True, mode='auto', period=1)\n",
    "\n",
    "train_generator = data_generator(DATASET_PATH, train_set, seq_len)\n",
    "valid_generator = data_generator(VAL_PATH, valid_set, seq_len)\n",
    "\n",
    "samples_per_epoch = math.ceil((len(train_set) - seq_len)/(seq_len*BATCH_SIZE))*BATCH_SIZE\n",
    "nb_val_samples = math.ceil((len(valid_set) - seq_len)/(seq_len*BATCH_SIZE))*BATCH_SIZE\n",
    "\n",
    "print(\"\\nSaving Model...\")\n",
    "\n",
    "model_json = model.to_json()\n",
    "with open(\"../../data/weights/speed_cnn_rnn/model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "#model.save_weights(\"model.h5\")\n",
    "print(\"Model Saved.\")\n",
    "\n",
    "\n",
    "history = model.fit_generator(train_generator,\n",
    "                              validation_data=valid_generator,\n",
    "                              samples_per_epoch=samples_per_epoch,\n",
    "                              nb_val_samples=nb_val_samples,\n",
    "                              nb_epoch=NUM_EPOCHS, verbose=1, \n",
    "                              callbacks=[lifecycle_callback, checkpoint])\n",
    "\n",
    "print(\"\\nTraining the model ended.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
