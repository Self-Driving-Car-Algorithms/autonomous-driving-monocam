{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if dataset loads\n",
    "import csv\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "DATASET_PATH = \"../../data/udacity_sim_data/\"\n",
    "VAL_PATH = \"../../data/track1/seq1/\"\n",
    "def load_dataset(file_path):\n",
    "    '''\n",
    "    Loads dataset in memory\n",
    "    '''\n",
    "    dataset = []\n",
    "    with open(file_path) as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        for line in reader:\n",
    "            try:\n",
    "                dataset.append({'center':line[0], 'left':line[1], 'right':line[2], 'steering':float(line[3]), \n",
    "                            'throttle':float(line[4]), 'brake':float(line[5]), 'speed':float(line[6])/15.0 - 1})\n",
    "            except:\n",
    "                continue\n",
    "    return dataset\n",
    "\n",
    "dataset = load_dataset(os.path.join(DATASET_PATH, \"driving_log.csv\"))\n",
    "val_dataset = load_dataset(os.path.join(VAL_PATH, \"driving_log.csv\"))\n",
    "\n",
    "print(\"Loaded {} samples from file {}\".format(len(dataset),DATASET_PATH))\n",
    "print(\"Loaded {} samples from file {}\".format(len(val_dataset),VAL_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "sample_file = os.path.join(DATASET_PATH, dataset[0]['center'].strip())\n",
    "INPUT_IMAGE_ROWS, INPUT_IMAGE_COLS, INPUT_IMAGE_CHANNELS = 160, 320, 3\n",
    "rows, cols, channels = 160, 320, 3\n",
    "\n",
    "seq_len = 5\n",
    "BATCH_SIZE = 5\n",
    "\n",
    "train_set = dataset\n",
    "valid_set = val_dataset\n",
    "\n",
    "print(\"train set has {} elements\".format(len(train_set)))\n",
    "print(\"valid set has {} elements\".format(len(valid_set)))\n",
    "\n",
    "def data_generator(path, dataset, seq_len):\n",
    "    batch_seq_images = np.zeros((BATCH_SIZE, seq_len, 160, 320, 3))\n",
    "    batch_seq_speed = np.zeros((BATCH_SIZE, seq_len, 1))\n",
    "    \n",
    "    while 1:\n",
    "        for i in range(BATCH_SIZE):\n",
    "            while 1:\n",
    "                index = np.random.randint(len(dataset))\n",
    "                if index + seq_len <= len(dataset):\n",
    "                    seq_speed = []\n",
    "                    seq_images = []\n",
    "                    for j in range(index, index + seq_len):\n",
    "                        seq_speed.append(dataset[j][\"speed\"])\n",
    "                        img = cv2.imread(path + dataset[j][\"center\"].strip())\n",
    "                        #img = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)[:, :, :]\n",
    "                        #img = np.asarray(img).reshape(160, 320, 3)\n",
    "                        seq_images.append(img)\n",
    "                    seq_speed = np.array(seq_speed)\n",
    "                    seq_images = np.array(seq_images)\n",
    "                    break\n",
    "                else:\n",
    "                    continue\n",
    "            batch_seq_images[i] = seq_images\n",
    "            batch_seq_speed[i] = seq_speed.reshape(seq_len, 1)\n",
    "            #batch_seq_steering_angles[i] = seq_steering_angles\n",
    "\n",
    "        # for ru\n",
    "        yield batch_seq_images, batch_seq_speed[:, -1, :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_gen = data_generator(DATASET_PATH, train_set, seq_len)\n",
    "x, y = next(sample_gen)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers.core import Lambda, Dense, Activation, Flatten, Dropout\n",
    "from keras.layers.convolutional import Cropping2D, Convolution2D, MaxPooling2D\n",
    "from keras.layers.advanced_activations import ELU\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.recurrent import GRU, LSTM\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.applications.vgg16 import VGG16\n",
    "\n",
    "print(\"\\nBuilding and compiling the model ...\")\n",
    "\n",
    "# import VGG\n",
    "vgg = VGG16(include_top=False, weights='imagenet', input_tensor=None, \n",
    "            input_shape=(rows, cols, 3), classes=1000)\n",
    "\n",
    "# access layers\n",
    "layer_dict = dict([(layer.name, layer) for layer in vgg.layers])\n",
    "\n",
    "\n",
    "# unfortunately this wont work because main model in rcnn_model. So for partial tuning,\n",
    "# develop complete model and transfer weights to do selective layering.\n",
    "# # declare non-trainable layers\n",
    "# for layer in cnn_model.layers[:17]:\n",
    "#     layer.trainable = False\n",
    "\n",
    "# define a CNN model out of VGG\n",
    "cnn_model = Model(input=vgg.input, output=vgg.get_layer(\"block5_pool\").output)\n",
    "\n",
    "# develop a combined model\n",
    "rcnn_model = Sequential()\n",
    "rcnn_model.add(TimeDistributed(cnn_model, input_shape=(None, 160, 320, 3)))\n",
    "rcnn_model.add(BatchNormalization(mode=0, axis=-1))\n",
    "rcnn_model.add(TimeDistributed(Flatten(name='flat')))\n",
    "rcnn_model.add(Activation('relu'))\n",
    "rcnn_model.add(Dropout(0.20))\n",
    "rcnn_model.add(GRU(64, return_sequences=False, init='glorot_uniform', inner_init='glorot_uniform', activation='relu',\n",
    "               name='GRU1'))\n",
    "rcnn_model.add(Dense(1, name='output')) \n",
    "\n",
    "rcnn_model.layers[0].trainable = False\n",
    "\n",
    "rcnn_model.summary()\n",
    "\n",
    "adam = Adam(lr=0.1)\n",
    "#rmsprop = RMSprop()\n",
    "rcnn_model.compile(loss='mean_squared_error', optimizer=adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.callbacks import Callback\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import math\n",
    "\n",
    "class LifeCycleCallBack(keras.callbacks.Callback):\n",
    "    \n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        pass\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        pass\n",
    "\n",
    "    def on_batch_begin(self, batch, logs={}):\n",
    "        pass\n",
    "    \n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        \n",
    "    def on_train_begin(self, logs={}):\n",
    "        print('Beginning training')\n",
    "        self.losses = []\n",
    "        \n",
    "    def on_train_end(self, logs={}):\n",
    "        print('Ending Training')\n",
    "\n",
    "\n",
    "NUM_EPOCHS = 250\n",
    "        \n",
    "lifecycle_callback = LifeCycleCallBack()\n",
    "\n",
    "tensorboard = keras.callbacks.TensorBoard(log_dir='../../data/logs/speed_cnn_rnn/', histogram_freq=1, \n",
    "                                          write_graph=True, write_images=True)\n",
    "\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(\"../../data/weights/speed_cnn_rnn/model.h5\", monitor='val_loss', verbose=0, \n",
    "                                             save_best_only=True, save_weights_only=True, mode='auto', period=1)\n",
    "\n",
    "train_generator = data_generator(DATASET_PATH, train_set, seq_len)\n",
    "valid_generator = data_generator(VAL_PATH, valid_set, seq_len)\n",
    "\n",
    "samples_per_epoch = math.ceil((len(train_set) - seq_len)/(seq_len*BATCH_SIZE))*BATCH_SIZE\n",
    "nb_val_samples = math.ceil((len(valid_set) - seq_len)/(seq_len*BATCH_SIZE))*BATCH_SIZE\n",
    "\n",
    "print(\"\\nSaving Model...\")\n",
    "\n",
    "model_json = rcnn_model.to_json()\n",
    "with open(\"../../data/weights/speed_cnn_rnn/model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "#model.save_weights(\"model.h5\")\n",
    "print(\"Model Saved.\")\n",
    "\n",
    "\n",
    "history = rcnn_model.fit_generator(train_generator,\n",
    "                              validation_data=valid_generator,\n",
    "                              samples_per_epoch=samples_per_epoch,\n",
    "                              nb_val_samples=nb_val_samples,\n",
    "                              nb_epoch=NUM_EPOCHS, verbose=1, \n",
    "                              callbacks=[lifecycle_callback, checkpoint])\n",
    "\n",
    "print(\"\\nTraining the model ended.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
