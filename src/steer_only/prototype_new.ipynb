{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import numpy as np\n",
    "DATASET_PATH = \"../../data/udacity_sim_data\"\n",
    "\n",
    "def load_dataset(file_path):\n",
    "    '''\n",
    "    Loads dataset in memory\n",
    "    '''\n",
    "    dataset = []\n",
    "    with open(file_path) as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        for line in reader:\n",
    "            try:\n",
    "                dataset.append({'center':line[0], 'left':line[1], 'right':line[2], 'steering':float(line[3]), \n",
    "                            'throttle':float(line[4]), 'brake':float(line[5]), 'speed':float(line[6])})\n",
    "            except:\n",
    "                continue\n",
    "    return dataset\n",
    "\n",
    "dataset = load_dataset(os.path.join(DATASET_PATH, \"driving_log.csv\"))\n",
    "print(\"Loaded {} samples from file {}\".format(len(dataset),DATASET_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    " \n",
    "print(\"\\nExploring the dataset ...\")\n",
    " \n",
    "# It plots the histogram of an arrray of angles: [0.0,0.1, ..., -0.1]\n",
    "def plot_steering_histogram(steerings, title, num_bins=50):\n",
    "    plt.hist(steerings, num_bins)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Steering Angles')\n",
    "    plt.ylabel('# Images')\n",
    "    plt.show()\n",
    " \n",
    "# It plots the histogram of an arrray of associative arrays of angles: [{'steering':0.1}, {'steering':0.2}, ..., {'steering':-0.1}]\n",
    "def plot_dataset_histogram(dataset, title, num_bins=50):\n",
    "    steerings = []\n",
    "    for item in dataset:\n",
    "        steerings.append(float(item['steering']))\n",
    "    plot_steering_histogram(steerings, title, num_bins)\n",
    " \n",
    "# Plot the histogram of steering angles before the image augmentation\n",
    "plot_dataset_histogram(dataset, 'Number of images per steering angle before image augmentation', num_bins=50)\n",
    "print(\"Exploring the dataset complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce 0.0 steering angles\n",
    "new_dataset = []\n",
    "for i in range(len(dataset)):\n",
    "    image_data = dataset[i]\n",
    "    steering_angle = image_data['steering']\n",
    "    if abs(steering_angle) <= 0.01 and np.random.randint(100) <= 80:\n",
    "        pass\n",
    "    else:\n",
    "        new_dataset.append(image_data)\n",
    "\n",
    "plot_dataset_histogram(new_dataset, 'Number of images per steering angle before image augmentation', num_bins=50)\n",
    "print(\"Exploring the dataset complete.\")\n",
    "print(len(new_dataset))\n",
    "dataset = new_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Partion the dataset\n",
    "\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "random.shuffle(dataset)\n",
    "\n",
    "train_set, test_set = train_test_split(dataset, test_size=0.05)\n",
    "\n",
    "train_set, valid_set = train_test_split(train_set, test_size=0.20)\n",
    "\n",
    "print(\"train set has {} elements\".format(len(train_set)))\n",
    "print(\"test set has {} elements\".format(len(test_set)))\n",
    "print(\"validation set has {} elements\".format(len(valid_set)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Augmentation\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from keras.preprocessing.image import *\n",
    "\n",
    "def image_preprocessing(img):\n",
    "    \"\"\"preproccesing training data to keep only S channel in HSV color space, and resize to 16X32\"\"\"\n",
    "\n",
    "    resized = cv2.resize((cv2.cvtColor(img, cv2.COLOR_RGB2HSV))[:,:,1],(32, 16))\n",
    "    return resized\n",
    "\n",
    "def flip_horizontal(image, steering_angle):\n",
    "    flipped_image = cv2.flip(image, 1)\n",
    "    steering_angle = -1 * steering_angle\n",
    "    return flipped_image, steering_angle\n",
    "\n",
    "def channel_shift(image, steering_angle, intensity=20, channel_axis=2):\n",
    "    channeled_image = random_channel_shift(image, intensity, channel_axis)\n",
    "    return channeled_image, steering_angle\n",
    "\n",
    "def random_transform(image, steering_angle):\n",
    "    image = image_preprocessing(image)\n",
    "    if np.random.random() < 0.5:\n",
    "        image, steering_angle = flip_horizontal(image, steering_angle)\n",
    "    \n",
    "    #image, steering_angle = channel_shift(image, steering_angle)\n",
    "\n",
    "    return image, steering_angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "BATCH_SIZE = 150\n",
    "\n",
    "sample_file = os.path.join(DATASET_PATH, dataset[0]['center'].strip())\n",
    "#INPUT_IMAGE_ROWS, INPUT_IMAGE_COLS, INPUT_IMAGE_CHANNELS = cv2.imread(sample_file).shape[:3]\n",
    "INPUT_IMAGE_ROWS, INPUT_IMAGE_COLS, INPUT_IMAGE_CHANNELS = 16, 32, 1\n",
    "\n",
    "def load_and_augment_image(image_data, side_camera_offset=0.20):\n",
    "    \n",
    "#     index = np.random.randint(3)\n",
    "    \n",
    "#     if index == 0:\n",
    "#         image_file = os.path.join(DATASET_PATH, image_data['left'].strip())\n",
    "#         angle_offset = side_camera_offset\n",
    "#     elif index == 1:\n",
    "#         image_file = os.path.join(DATASET_PATH, image_data['center'].strip())\n",
    "#         angle_offset = 0.\n",
    "#     elif index == 2:\n",
    "#         image_file = os.path.join(DATASET_PATH, image_data['right'].strip())\n",
    "#         angle_offset = - side_camera_offset\n",
    "    \n",
    "#     steering_angle = image_data['steering'] + angle_offset\n",
    "    steering_angle = image_data['steering']\n",
    "    image_file = os.path.join(DATASET_PATH, image_data['center'].strip())\n",
    "    image = cv2.imread(image_file)\n",
    "    #image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    image, steering_angle = random_transform(image, steering_angle)\n",
    "    return image, steering_angle\n",
    "\n",
    "augmented_steering_angles = []\n",
    "image_index = 0\n",
    "\n",
    "def generate_batch_data(dataset, batch_size=BATCH_SIZE):\n",
    "    \n",
    "    global augmented_steering_angles\n",
    "    global image_index\n",
    "\n",
    "    batch_images = np.zeros((batch_size, INPUT_IMAGE_ROWS, INPUT_IMAGE_COLS, INPUT_IMAGE_CHANNELS))\n",
    "    batch_steering_angles = np.zeros(batch_size)\n",
    "\n",
    "    while 1:\n",
    "        for batch_index in range(batch_size):\n",
    "\n",
    "            #image_index = np.random.randint(len(dataset))\n",
    "            if image_index >= len(dataset):\n",
    "                image_index = 0\n",
    "            image_data = dataset[image_index]\n",
    "\n",
    "            try:\n",
    "                image, steering_angle = load_and_augment_image(image_data)\n",
    "                augmented_steering_angles.append(steering_angle)\n",
    "                batch_images[batch_index] = image.reshape([16, 32, 1])\n",
    "                batch_steering_angles[batch_index] = steering_angle\n",
    "                image_index = image_index + 1\n",
    "\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "        yield batch_images, batch_steering_angles\n",
    "\n",
    "\n",
    "image, _ = load_and_augment_image(dataset[0])\n",
    "\n",
    "print(image.shape)\n",
    "\n",
    "plt.imshow(image)\n",
    "plt.xticks([]), plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_gen = generate_batch_data(train_set)\n",
    "for i in range(50):\n",
    "    next(sample_gen)\n",
    "plot_steering_histogram(augmented_steering_angles, \"augmented_data\", num_bins=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation dataset\n",
    "\n",
    "# for image_data in valid_set:\n",
    "#     image, steering_angle = load_and_augment_image(image_data)\n",
    "#     X_valid.append(image)\n",
    "#     Y_valid.append(steering_angle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers.core import Lambda, Dense, Activation, Flatten, Dropout\n",
    "from keras.layers.convolutional import Cropping2D, Convolution2D, MaxPooling2D\n",
    "from keras.layers.advanced_activations import ELU\n",
    "from keras.layers.noise import GaussianNoise\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "print(\"\\nBuilding and compiling the model ...\")\n",
    "\n",
    "model = Sequential()\n",
    "# Preprocess incoming data, centered around zero with small standard deviation \n",
    "model.add(Lambda(lambda x: (x / 127.5) - 1.0, input_shape=(INPUT_IMAGE_ROWS, INPUT_IMAGE_COLS, INPUT_IMAGE_CHANNELS)))\n",
    "# Block - conv\n",
    "model.add(Convolution2D(2, 3, 3, border_mode='valid', init='glorot_uniform', activation='relu', name='Conv1'))\n",
    "# Block - conv\n",
    "# model.add(Convolution2D(3, 5, 5, border_mode='valid', subsample=[2,2], init='glorot_uniform', activation='elu', name='Conv2'))\n",
    "# Block - conv\n",
    "# model.add(Convolution2D(1, 3, 3, border_mode='valid', subsample=[2,2], init='glorot_uniform', activation='elu', name='Conv3'))\n",
    "# Block - conv\n",
    "# model.add(Convolution2D(32, 5, 5, border_mode='valid', subsample=[2,2], init='glorot_uniform', activation='elu', name='Conv4'))\n",
    "# Block - flatten\n",
    "model.add(MaxPooling2D((4,4),(4,4),'valid', name='pool1'))\n",
    "model.add(Dropout(0.20))\n",
    "model.add(Flatten())\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(ELU())\n",
    "\n",
    "# Block - fully connected\n",
    "# model.add(Dense(256, activation='elu', name='FC1'))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(ELU())\n",
    "# model.add(Dense(8, activation='elu', name='FC2'))\n",
    "# model.add(ELU())\n",
    "# Block - output\n",
    "model.add(Dense(1, name='output')) \n",
    "model.summary()\n",
    "\n",
    "adam = Adam(lr=0.001)\n",
    "model.compile(loss='mean_squared_error', optimizer=adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUGMENTATION_FACTOR = 1\n",
    "NUM_EPOCHS = 100\n",
    "\n",
    "import keras\n",
    "from keras.callbacks import Callback\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import math\n",
    "\n",
    "class LifeCycleCallBack(keras.callbacks.Callback):\n",
    "    \n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        pass\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        pass\n",
    "    \n",
    "    def on_batch_begin(self, batch, logs={}):\n",
    "        pass\n",
    "    \n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        \n",
    "    def on_train_begin(self, logs={}):\n",
    "        print('Beginning training')\n",
    "        self.losses = []\n",
    "        \n",
    "    def on_train_end(self, logs={}):\n",
    "        print('Ending Training')\n",
    "\n",
    "        \n",
    "def compute_samples_per_epoch(array_size, batch_size):\n",
    "    num_batches = math.ceil(array_size / batch_size)\n",
    "    samples_per_epoch = num_batches * batch_size\n",
    "    return samples_per_epoch\n",
    "\n",
    "lifecycle_callback = LifeCycleCallBack()\n",
    "tensorboard = keras.callbacks.TensorBoard(log_dir='../../data/logs/steer_only/', histogram_freq=1, \n",
    "                                          write_graph=True, write_images=True)\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(\"../../data/weights/steer_only/model.h5\", monitor='val_loss', verbose=0, \n",
    "                                             save_best_only=True, save_weights_only=True, mode='auto', period=1)\n",
    "\n",
    "\n",
    "\n",
    "train_generator = generate_batch_data(train_set, BATCH_SIZE)\n",
    "valid_generator = generate_batch_data(valid_set, BATCH_SIZE)\n",
    "\n",
    "samples_per_epoch = compute_samples_per_epoch((len(train_set)*AUGMENTATION_FACTOR), BATCH_SIZE) \n",
    "nb_val_samples = compute_samples_per_epoch((len(valid_set)*AUGMENTATION_FACTOR), BATCH_SIZE) \n",
    " \n",
    "history = model.fit_generator(train_generator, \n",
    "                              validation_data = valid_generator,\n",
    "                              samples_per_epoch = samples_per_epoch, \n",
    "                              nb_val_samples = nb_val_samples,\n",
    "                              nb_epoch = NUM_EPOCHS, verbose=1, \n",
    "                              callbacks=[lifecycle_callback, tensorboard, checkpoint])\n",
    "\n",
    "print(\"\\nTraining the model ended.\")\n",
    "\n",
    "print(\"\\nSaving Model...\")\n",
    "\n",
    "model_json = model.to_json()\n",
    "with open(\"../../data/weights/steer_only/model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "#model.save_weights(\"model.h5\")\n",
    "print(\"Model Saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_json = model.to_json()\n",
    "with open(\"../../data/weights/steer_only/model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "#model.save_weights(\"model.h5\")\n",
    "print(\"Model Saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool\n",
    "\n",
    "\n",
    "def image_preprocessing(img):\n",
    "    \"\"\"preproccesing training data to keep only S channel in HSV color space, and resize to 16X32\"\"\"\n",
    "\n",
    "    resized = cv2.resize((cv2.cvtColor(img, cv2.COLOR_RGB2HSV))[:,:,1],(img_cols,img_rows))\n",
    "    return resized\n",
    "\n",
    "def extract(i):\n",
    "    X_ = []\n",
    "    y_ = []\n",
    "\n",
    "    img_path = logs[i][0]\n",
    "    img_path = data_folder+'IMG'+(img_path.split('IMG')[1]).strip()\n",
    "    img = plt.imread(img_path)\n",
    "    X_.append(image_preprocessing(img))\n",
    "    y_.append(float(logs[i][3]))\n",
    "\n",
    "    img_path = logs[i][1]\n",
    "    img_path = data_folder+'IMG'+(img_path.split('IMG')[1]).strip()\n",
    "    img = plt.imread(img_path)\n",
    "    X_.append(image_preprocessing(img))\n",
    "    y_.append(float(logs[i][3]) + delta_)\n",
    "\n",
    "    img_path = logs[i][2]\n",
    "    img_path = data_folder+'IMG'+(img_path.split('IMG')[1]).strip()\n",
    "    img = plt.imread(img_path)\n",
    "    X_.append(image_preprocessing(img))\n",
    "    y_.append(float(logs[i][3]) - delta_)\n",
    "\n",
    "    return X_, y_\n",
    "\n",
    "def load_data(X,y,data_folder,delta=0.08):\n",
    "    \"\"\"function to load training data\"\"\"\n",
    "\n",
    "    global logs\n",
    "    global delta_\n",
    "\n",
    "    delta_ = delta\n",
    "\n",
    "    log_path = data_folder + 'driving_log.csv'\n",
    "    logs = []\n",
    "\n",
    "    # load logs\n",
    "    with open(log_path,'rt') as f:\n",
    "        reader = csv.reader(f)\n",
    "        for line in reader:\n",
    "            logs.append(line)\n",
    "        log_labels = logs.pop(0)\n",
    "\n",
    "    pool = Pool(processes=6)\n",
    "\n",
    "    for X_, y_ in tqdm(pool.imap_unordered(extract, range(len(logs))), total=len(logs)):\n",
    "        X.extend(X_)\n",
    "        y.extend(y_)\n",
    "    pool.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8036/8036 [00:23<00:00, 340.12it/s]\n"
     ]
    }
   ],
   "source": [
    "img_rows = 16\n",
    "img_cols = 32\n",
    "\n",
    "#data path\n",
    "data_folder = '../../data/udacity_official/'\n",
    "\n",
    "#batch size and epoch\n",
    "batch_size=128\n",
    "nb_epoch=7\n",
    "\n",
    "data={}\n",
    "data['features'] = []\n",
    "data['labels'] = []\n",
    "\n",
    "load_data(data['features'], data['labels'], data_folder, 0.3)\n",
    "\n",
    "X_train = np.array(data['features']).astype('float32')\n",
    "y_train = np.array(data['labels']).astype('float32')\n",
    "\n",
    "\n",
    "# horizonal reflection to agument the data\n",
    "X_train = np.append(X_train, X_train[:,:,::-1], axis=0)\n",
    "y_train = np.append(y_train, -y_train, axis=0)\n",
    "\n",
    "# split train and validation\n",
    "X_train, y_train = shuffle(X_train, y_train)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, random_state=0, test_size=0.1)\n",
    "\n",
    "# reshape to have correct dimension\n",
    "X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\n",
    "X_val = X_val.reshape(X_val.shape[0], img_rows, img_cols, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"images_saturation.npy\", X_train)\n",
    "np.save(\"steering.npy\", y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building and compiling the model ...\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "lambda_1 (Lambda)                (None, 16, 32, 1)     0           lambda_input_1[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "Conv1 (Convolution2D)            (None, 14, 30, 2)     20          lambda_1[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "pool1 (MaxPooling2D)             (None, 3, 7, 2)       0           Conv1[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 3, 7, 2)       0           pool1[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 42)            0           dropout_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "output (Dense)                   (None, 1)             43          flatten_1[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 63\n",
      "Trainable params: 63\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers.core import Lambda, Dense, Activation, Flatten, Dropout\n",
    "from keras.layers.convolutional import Cropping2D, Convolution2D, MaxPooling2D\n",
    "from keras.layers.advanced_activations import ELU\n",
    "from keras.layers.noise import GaussianNoise\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "print(\"\\nBuilding and compiling the model ...\")\n",
    "\n",
    "model = Sequential()\n",
    "# Preprocess incoming data, centered around zero with small standard deviation \n",
    "model.add(Lambda(lambda x: (x / 127.5) - 1.0, input_shape=(img_rows, img_cols, 1)))\n",
    "# Block - conv\n",
    "model.add(Convolution2D(2, 3, 3, border_mode='valid', init='glorot_uniform', activation='relu', name='Conv1'))\n",
    "# Block - conv\n",
    "# model.add(Convolution2D(3, 5, 5, border_mode='valid', subsample=[2,2], init='glorot_uniform', activation='elu', name='Conv2'))\n",
    "# Block - conv\n",
    "# model.add(Convolution2D(1, 3, 3, border_mode='valid', subsample=[2,2], init='glorot_uniform', activation='elu', name='Conv3'))\n",
    "# Block - conv\n",
    "# model.add(Convolution2D(32, 5, 5, border_mode='valid', subsample=[2,2], init='glorot_uniform', activation='elu', name='Conv4'))\n",
    "# Block - flatten\n",
    "model.add(MaxPooling2D((4,4),(4,4),'valid', name='pool1'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(ELU())\n",
    "\n",
    "# Block - fully connected\n",
    "# model.add(Dense(256, activation='elu', name='FC1'))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(ELU())\n",
    "# model.add(Dense(8, activation='elu', name='FC2'))\n",
    "# model.add(ELU())\n",
    "# Block - output\n",
    "model.add(Dense(1, name='output')) \n",
    "model.summary()\n",
    "\n",
    "adam = Adam(lr=0.001)\n",
    "model.compile(loss='mean_squared_error', optimizer=adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 43394 samples, validate on 4822 samples\n",
      "Epoch 1/7\n",
      "43394/43394 [==============================] - 6s - loss: 0.3689 - val_loss: 0.0678\n",
      "Epoch 2/7\n",
      "43394/43394 [==============================] - 6s - loss: 0.0967 - val_loss: 0.0605\n",
      "Epoch 3/7\n",
      "43394/43394 [==============================] - 6s - loss: 0.0746 - val_loss: 0.0574\n",
      "Epoch 4/7\n",
      "43394/43394 [==============================] - 7s - loss: 0.0673 - val_loss: 0.0550\n",
      "Epoch 5/7\n",
      "43394/43394 [==============================] - 7s - loss: 0.0628 - val_loss: 0.0526\n",
      "Epoch 6/7\n",
      "43394/43394 [==============================] - 7s - loss: 0.0592 - val_loss: 0.0500\n",
      "Epoch 7/7\n",
      "43394/43394 [==============================] - 7s - loss: 0.0566 - val_loss: 0.0478\n",
      "\n",
      "Training the model ended.\n",
      "\n",
      "Saving Model...\n",
      "Model Saved.\n"
     ]
    }
   ],
   "source": [
    "AUGMENTATION_FACTOR = 1\n",
    "nb_epoch=7\n",
    "\n",
    "import keras\n",
    "from keras.callbacks import Callback\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import math\n",
    "\n",
    "\n",
    "tensorboard = keras.callbacks.TensorBoard(log_dir='../../data/logs/steer_only/', histogram_freq=1, \n",
    "                                          write_graph=True, write_images=True)\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(\"../../data/weights/steer_only/model.h5\", monitor='val_loss', verbose=0, \n",
    "                                             save_best_only=True, save_weights_only=True, mode='auto', period=1)\n",
    "\n",
    " \n",
    "history = model.fit(X_train, y_train, \n",
    "                  batch_size=batch_size,\n",
    "                  nb_epoch=nb_epoch, verbose=1,\n",
    "                  validation_data=(X_val, y_val))\n",
    "\n",
    "print(\"\\nTraining the model ended.\")\n",
    "\n",
    "print(\"\\nSaving Model...\")\n",
    "\n",
    "model_json = model.to_json()\n",
    "with open(\"../../data/weights/steer_only/model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "model.save_weights(\"model.h5\")\n",
    "print(\"Model Saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
