{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if dataset loads\n",
    "\n",
    "import csv\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import keras\n",
    "\n",
    "DATASET_PATH = \"../../data/udacity_sim_data/\"\n",
    "VAL_PATH = \"../../data/track1/seq1/\"\n",
    "def load_dataset(file_path):\n",
    "    '''\n",
    "    Loads dataset in memory\n",
    "    '''\n",
    "    dataset = []\n",
    "    with open(file_path) as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        for line in reader:\n",
    "            try:\n",
    "                dataset.append({'center':line[0], 'left':line[1], 'right':line[2], 'steering':float(line[3]), \n",
    "                            'throttle':float(line[4]), 'brake':float(line[5]), 'speed':float(line[6])})\n",
    "            except:\n",
    "                continue\n",
    "    return dataset\n",
    "\n",
    "dataset = load_dataset(os.path.join(DATASET_PATH, \"driving_log.csv\"))\n",
    "val_dataset = load_dataset(os.path.join(VAL_PATH, \"driving_log.csv\"))\n",
    "\n",
    "print(\"Loaded {} samples from file {}\".format(len(dataset),DATASET_PATH))\n",
    "print(\"Loaded {} samples from file {}\".format(len(val_dataset),VAL_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "sample_file = os.path.join(DATASET_PATH, dataset[0]['center'].strip())\n",
    "INPUT_IMAGE_ROWS, INPUT_IMAGE_COLS, INPUT_IMAGE_CHANNELS = 160, 320, 1\n",
    "\n",
    "seq_len = 5\n",
    "BATCH_SIZE = 10\n",
    "\n",
    "train_set = dataset\n",
    "valid_set = val_dataset\n",
    "\n",
    "print(\"train set has {} elements\".format(len(train_set)))\n",
    "print(\"valid set has {} elements\".format(len(valid_set)))\n",
    "\n",
    "def data_generator(path, dataset, seq_len):\n",
    "    batch_seq_images = np.zeros((BATCH_SIZE, seq_len, 160, 320, 1))\n",
    "    batch_seq_steering_angles = np.zeros((BATCH_SIZE, seq_len, 1))\n",
    "    \n",
    "    while 1:\n",
    "        for i in range(BATCH_SIZE):\n",
    "            while 1:\n",
    "                index = np.random.randint(len(dataset))\n",
    "                if index + seq_len <= len(dataset):\n",
    "                    seq_steering_angles = []\n",
    "                    seq_images = []\n",
    "                    for j in range(index, index + seq_len):\n",
    "                        seq_steering_angles.append(dataset[j][\"steering\"])\n",
    "                        img = cv2.imread(path + dataset[j][\"center\"].strip())\n",
    "                        img = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)[:, :, 1]\n",
    "                        img = np.asarray(img).reshape(160, 320, 1)\n",
    "                        seq_images.append(img)\n",
    "                    seq_steering_angles = np.array(seq_steering_angles)\n",
    "                    seq_images = np.array(seq_images)\n",
    "                    break\n",
    "                else:\n",
    "                    continue\n",
    "            batch_seq_images[i] = seq_images\n",
    "            batch_seq_steering_angles[i] = seq_steering_angles.reshape(seq_len, 1)\n",
    "            #batch_seq_steering_angles[i] = seq_steering_angles\n",
    "\n",
    "        # for ru\n",
    "        yield batch_seq_images, batch_seq_steering_angles[:, -1, :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers.core import Lambda, Dense, Activation, Flatten, Dropout\n",
    "from keras.layers.convolutional import Cropping2D, Convolution2D, MaxPooling2D\n",
    "from keras.layers.advanced_activations import ELU\n",
    "from keras.layers.recurrent import GRU, LSTM\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "\n",
    "print(\"\\nBuilding and compiling the model ...\")\n",
    "\n",
    "model = Sequential()\n",
    "# Preprocess incoming data, centered around zero with small standard deviation \n",
    "model.add(TimeDistributed(Lambda(lambda x: (x / 127.5) - 1.0), input_shape=(None, INPUT_IMAGE_ROWS, INPUT_IMAGE_COLS, INPUT_IMAGE_CHANNELS)))\n",
    "# Block - conv\n",
    "model.add(TimeDistributed(Convolution2D(8, 11, 11, border_mode='valid', subsample=[2, 2], init='glorot_uniform', activation='relu', name='Conv1')))\n",
    "# Block - conv\n",
    "model.add(TimeDistributed(Convolution2D(16, 5, 5, border_mode='valid', subsample=[2, 2], init='glorot_uniform', activation='relu', name='Conv2')))\n",
    "# Block - conv\n",
    "model.add(TimeDistributed(Convolution2D(32, 5, 5, border_mode='valid', subsample=[2, 2], init='glorot_uniform', activation='relu', name='Conv3')))\n",
    "# Block - conv\n",
    "#model.add(Convolution2D(1, 3, 3, border_mode='valid', init='glorot_uniform', activation='relu', name='Conv4'))\n",
    "# Block - flatten\n",
    "#model.add(TimeDistributed(MaxPooling2D((4,4),(4,4),'valid', name='pool1')))\n",
    "model.add(TimeDistributed(Flatten(name='flat')))\n",
    "model.add(Activation('relu'))\n",
    "model.add(TimeDistributed(Dropout(0.20)))\n",
    "model.add(GRU(16, return_sequences=False, init='glorot_uniform', inner_init='glorot_uniform', activation='relu',\n",
    "             name='GRU1'))\n",
    "#model.add(Dropout(0.20))\n",
    "model.add(Dense(1, name='output')) \n",
    "model.summary()\n",
    "\n",
    "adam = Adam(lr=0.0001)\n",
    "#rmsprop = RMSprop()\n",
    "model.compile(loss='mean_squared_error', optimizer=adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.callbacks import Callback\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import math\n",
    "\n",
    "class LifeCycleCallBack(keras.callbacks.Callback):\n",
    "    \n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        pass\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        pass\n",
    "\n",
    "    def on_batch_begin(self, batch, logs={}):\n",
    "        pass\n",
    "    \n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        \n",
    "    def on_train_begin(self, logs={}):\n",
    "        print('Beginning training')\n",
    "        self.losses = []\n",
    "        \n",
    "    def on_train_end(self, logs={}):\n",
    "        print('Ending Training')\n",
    "\n",
    "\n",
    "NUM_EPOCHS = 250\n",
    "        \n",
    "lifecycle_callback = LifeCycleCallBack()\n",
    "\n",
    "tensorboard = keras.callbacks.TensorBoard(log_dir='../../data/logs/steer_cnn_rnn/', histogram_freq=1, \n",
    "                                          write_graph=True, write_images=True)\n",
    "\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(\"../../data/weights/steer_cnn_rnn/model.h5\", monitor='val_loss', verbose=0, \n",
    "                                             save_best_only=True, save_weights_only=True, mode='auto', period=1)\n",
    "\n",
    "train_generator = data_generator(DATASET_PATH, train_set, seq_len)\n",
    "valid_generator = data_generator(VAL_PATH, valid_set, seq_len)\n",
    "\n",
    "samples_per_epoch = math.ceil((len(train_set) - seq_len)/(seq_len*BATCH_SIZE))\n",
    "nb_val_samples = math.ceil((len(valid_set) - seq_len)/(seq_len*BATCH_SIZE))*3\n",
    "\n",
    "print(\"\\nSaving Model...\")\n",
    "\n",
    "model_json = model.to_json()\n",
    "with open(\"../../data/weights/steer_cnn_rnn/model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "#model.save_weights(\"model.h5\")\n",
    "print(\"Model Saved.\")\n",
    "\n",
    "\n",
    "history = model.fit_generator(train_generator,\n",
    "                              validation_data=valid_generator,\n",
    "                              samples_per_epoch=samples_per_epoch,\n",
    "                              nb_val_samples=nb_val_samples,\n",
    "                              nb_epoch=NUM_EPOCHS, verbose=1, \n",
    "                              callbacks=[lifecycle_callback, tensorboard, checkpoint])\n",
    "\n",
    "print(\"\\nTraining the model ended.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
