{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 45999 samples from file ../../data/udacity_sim_data/\n"
     ]
    }
   ],
   "source": [
    "#Check if dataset loads\n",
    "\n",
    "import csv\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import keras\n",
    "\n",
    "DATASET_PATH = \"../../data/udacity_sim_data/\"\n",
    "\n",
    "def load_dataset(file_path):\n",
    "    '''\n",
    "    Loads dataset in memory\n",
    "    '''\n",
    "    dataset = []\n",
    "    with open(file_path) as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        for line in reader:\n",
    "            try:\n",
    "                dataset.append({'center':line[0], 'left':line[1], 'right':line[2], 'steering':float(line[3]), \n",
    "                            'throttle':float(line[4]), 'brake':float(line[5]), 'speed':float(line[6])})\n",
    "            except:\n",
    "                continue\n",
    "    return dataset\n",
    "\n",
    "dataset = load_dataset(os.path.join(DATASET_PATH, \"driving_log.csv\"))\n",
    "print(\"Loaded {} samples from file {}\".format(len(dataset),DATASET_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Exploring the dataset ...\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbEAAAEWCAYAAADoyannAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZpXlJGZmZpX1/wHeIIYGIZWuzwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fec6bfbf7b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exploring the dataset complete.\n"
     ]
    }
   ],
   "source": [
    "# Plot data distribution\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "print(\"\\nExploring the dataset ...\")\n",
    " \n",
    "# It plots the histogram of an arrray of angles: [0.0,0.1, ..., -0.1]\n",
    "def plot_steering_histogram(steerings, title, num_bins=100):\n",
    "    plt.hist(steerings, num_bins)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Steering Angles')\n",
    "    plt.ylabel('# Images')\n",
    "    plt.show()\n",
    " \n",
    "# It plots the histogram of an arrray of associative arrays of angles: [{'steering':0.1}, {'steering':0.2}, ..., {'steering':-0.1}]\n",
    "def plot_dataset_histogram(dataset, title, num_bins=100):\n",
    "    steerings = []\n",
    "    for item in dataset:\n",
    "        steerings.append( float(item['steering']) )\n",
    "    plot_steering_histogram(steerings, title, num_bins)\n",
    " \n",
    "# Plot the histogram of steering angles before the image augmentation\n",
    "plot_dataset_histogram(dataset, 'Number of images per steering angle before image augmentation', num_bins=100)\n",
    "print(\"Exploring the dataset complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "lambda_2 (Lambda)                (None, 160, 320, 3)   0           lambda_input_2[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "Conv1 (Convolution2D)            (None, 38, 78, 8)     2912        lambda_2[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "Conv2 (Convolution2D)            (None, 17, 37, 16)    3216        Conv1[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "Conv3 (Convolution2D)            (None, 7, 17, 32)     12832       Conv2[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "Conv4 (Convolution2D)            (None, 2, 7, 32)      25632       Conv3[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)              (None, 448)           0           Conv4[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)              (None, 448)           0           flatten_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "elu_4 (ELU)                      (None, 448)           0           dropout_3[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "FC1 (Dense)                      (None, 256)           114944      elu_4[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)              (None, 256)           0           FC1[0][0]                        \n",
      "____________________________________________________________________________________________________\n",
      "elu_5 (ELU)                      (None, 256)           0           dropout_4[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "FC2 (Dense)                      (None, 64)            16448       elu_5[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "elu_6 (ELU)                      (None, 64)            0           FC2[0][0]                        \n",
      "____________________________________________________________________________________________________\n",
      "output (Dense)                   (None, 1)             65          elu_6[0][0]                      \n",
      "====================================================================================================\n",
      "Total params: 176,049\n",
      "Trainable params: 176,049\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "GRU1 (GRU)                       (None, None, 32)      9312        gru_input_2[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, None, 32)      0           GRU1[0][0]                       \n",
      "____________________________________________________________________________________________________\n",
      "output (Dense)                   (None, None, 1)       33          dropout_1[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 9,345\n",
      "Trainable params: 9,345\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "timedistributed_2 (TimeDistribut (None, None, 64)      175984      timedistributed_input_2[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "model_4 (Model)                  (None, None, 1)       9345        timedistributed_2[0][0]          \n",
      "====================================================================================================\n",
      "Total params: 185,329\n",
      "Trainable params: 185,329\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "Tensor(\"timedistributed_input_2:0\", shape=(?, ?, 160, 320, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Load CNN model with weights. Cut it till certain layer\n",
    "# Make the CNN model time distributed\n",
    "# Load RNN model with weights\n",
    "# Define new model that combines this two and compile\n",
    "\n",
    "from keras.models import load_model, Model\n",
    "from keras.models import model_from_json\n",
    "from keras.models import Sequential\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.optimizers import Adam\n",
    "from tqdm import tqdm\n",
    "\n",
    "CNN_MODEL_PATH = \"../../data/weights/steer_only/model.json\"\n",
    "RNN_MODEL_PATH = \"../../data/weights/steer_rnn_best/model.json\"\n",
    "\n",
    "with open(CNN_MODEL_PATH, 'r') as jfile:\n",
    "    cnn_model = model_from_json(jfile.read())\n",
    "\n",
    "with open(RNN_MODEL_PATH, 'r') as jfile:\n",
    "    rnn_model = model_from_json(jfile.read())\n",
    "\n",
    "cnn_weights = CNN_MODEL_PATH.replace('json', 'h5')\n",
    "cnn_model.load_weights(cnn_weights)\n",
    "cnn_model.summary()\n",
    "\n",
    "rnn_weights = RNN_MODEL_PATH.replace('json', 'h5')\n",
    "rnn_model.load_weights(rnn_weights)\n",
    "rnn_model.summary()\n",
    "\n",
    "cnn_model = Model(input=cnn_model.input, output=cnn_model.get_layer(\"FC2\").output)\n",
    "rnn_model = Model(input=rnn_model.input, output=rnn_model.get_layer(\"output\").output)\n",
    "\n",
    "# print(cnn_model.input)\n",
    "# print(cnn_model.output)\n",
    "# print(rnn_model.input)\n",
    "# print(rnn_model.output)\n",
    "\n",
    "rcnn_model = Sequential()\n",
    "rcnn_model.add(TimeDistributed(cnn_model, input_shape=(None, 160, 320, 3)))\n",
    "rcnn_model.add(rnn_model)\n",
    "\n",
    "adam = Adam(lr=0.0001)\n",
    "rcnn_model.compile(loss='mean_squared_error', optimizer=adam)\n",
    "\n",
    "rcnn_model.summary()\n",
    "\n",
    "print(rcnn_model.input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Not using this block of code. Ignore\n",
    "\n",
    "\n",
    "# Instead of loading CNN model, define new CNN model with same layer names so that weights get loaded\n",
    "# But while defining new CNN model, add time distribution to each layer\n",
    "\n",
    "# But currently, names of produced model is not matching with saved model weights\n",
    "\n",
    "from keras.models import load_model, Model\n",
    "from keras.models import model_from_json\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Lambda, Dense, Activation, Flatten, Dropout\n",
    "from keras.layers.convolutional import Cropping2D, Convolution2D\n",
    "from keras.layers.advanced_activations import ELU\n",
    "from keras.layers.noise import GaussianNoise\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.optimizers import Adam\n",
    "from tqdm import tqdm\n",
    "\n",
    "CNN_MODEL_PATH = \"../../data/weights/steer_only/model.json\"\n",
    "RNN_MODEL_PATH = \"../../data/weights/steer_rnn_best/model.json\"\n",
    "\n",
    "with open(RNN_MODEL_PATH, 'r') as jfile:\n",
    "    rnn_model = model_from_json(jfile.read())\n",
    "\n",
    "rnn_weights = RNN_MODEL_PATH.replace('json', 'h5')\n",
    "rnn_model.load_weights(rnn_weights)\n",
    "rnn_model.summary()\n",
    "\n",
    "\n",
    "cnn_model = Sequential()\n",
    "# Preprocess incoming data, centered around zero with small standard deviation \n",
    "cnn_model.add(TimeDistributed(Lambda(lambda x: (x / 127.5) - 1.0), input_shape=(None, 160, 320, 3)))\n",
    "# Block - conv\n",
    "cnn_model.add(TimeDistributed(Convolution2D(8, 11, 11, border_mode='valid', subsample=[4,4], init='glorot_uniform', \n",
    "                            activation='elu', name='Conv1')))\n",
    "# Block - conv\n",
    "cnn_model.add(TimeDistributed(Convolution2D(16, 5, 5, border_mode='valid', subsample=[2,2], init='glorot_uniform', \n",
    "                            activation='elu', name='Conv2')))\n",
    "# Block - conv\n",
    "cnn_model.add(TimeDistributed(Convolution2D(32, 5, 5, border_mode='valid', subsample=[2,2], init='glorot_uniform', \n",
    "                            activation='elu', name='Conv3')))\n",
    "# Block - conv\n",
    "cnn_model.add(TimeDistributed(Convolution2D(32, 5, 5, border_mode='valid', subsample=[2,2], init='glorot_uniform', \n",
    "                            activation='elu', name='Conv4')))\n",
    "# Block - flatten\n",
    "cnn_model.add(TimeDistributed(Flatten()))\n",
    "cnn_model.add(TimeDistributed(Dropout(0.2)))\n",
    "cnn_model.add(TimeDistributed(ELU()))\n",
    "# Block - fully connected\n",
    "cnn_model.add(TimeDistributed(Dense(256, activation='elu', name='FC1')))\n",
    "cnn_model.add(TimeDistributed(Dropout(0.2)))\n",
    "cnn_model.add(TimeDistributed(ELU()))\n",
    "cnn_model.add(TimeDistributed(Dense(64, activation='elu', name='FC2')))\n",
    "cnn_model.add(TimeDistributed(ELU()))\n",
    "# Block - output\n",
    "cnn_model.add(TimeDistributed(Dense(1, activation='linear', name='output'))) \n",
    "cnn_model.summary()\n",
    "\n",
    "cnn_weights = CNN_MODEL_PATH.replace('json', 'h5')\n",
    "cnn_model.load_weights(cnn_weights)\n",
    "\n",
    "cnn_model = Model(input=cnn_model.input, output=cnn_model.get_layer(\"FC2\").output)\n",
    "rnn_model = Model(input=rnn_model.input, output=rnn_model.get_layer(\"output\").output)\n",
    "\n",
    "# print(cnn_model.input)\n",
    "# print(cnn_model.output)\n",
    "# print(rnn_model.input)\n",
    "# print(rnn_model.output)\n",
    "\n",
    "rcnn_model = Sequential()\n",
    "rcnn_model.add(TimeDistributed(cnn_model, input_shape=(None, 160, 320, 3)))\n",
    "rcnn_model.add(rnn_model)\n",
    "\n",
    "adam = Adam(lr=0.0001)\n",
    "rcnn_model.compile(loss='mean_squared_error', optimizer=adam)\n",
    "\n",
    "rcnn_model.summary()\n",
    "\n",
    "print(rcnn_model.input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set has 39099 elements\n",
      "test set has 6900 elements\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "seq_len = 30\n",
    "BATCH_SIZE = 40\n",
    "\n",
    "train_set, test_set = train_test_split(dataset, test_size=0.15)\n",
    "\n",
    "print(\"train set has {} elements\".format(len(train_set)))\n",
    "print(\"test set has {} elements\".format(len(test_set)))\n",
    "\n",
    "def data_generator(dataset, seq_len):\n",
    "    batch_seq_images = np.zeros((BATCH_SIZE, seq_len, 160, 320 ,3))\n",
    "    batch_seq_steering_angles = np.zeros((BATCH_SIZE, seq_len, 1))\n",
    "    \n",
    "    while 1:\n",
    "        for i in range(BATCH_SIZE):\n",
    "            while 1:\n",
    "                index = np.random.randint(len(dataset))\n",
    "                if index + seq_len <= len(dataset):\n",
    "                    seq_steering_angles = []\n",
    "                    seq_images = []\n",
    "                    for j in range(index, index + seq_len):\n",
    "                        seq_steering_angles.append(dataset[j][\"steering\"])\n",
    "                        img = cv2.imread(DATASET_PATH + dataset[j][\"center\"].strip())\n",
    "                        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                        seq_images.append(img)\n",
    "                    seq_steering_angles = np.array(seq_steering_angles)\n",
    "                    seq_images = np.array(seq_images)\n",
    "                    break\n",
    "                else:\n",
    "                    continue\n",
    "            batch_seq_images[i] = seq_images\n",
    "            batch_seq_steering_angles[i] = seq_steering_angles.reshape(30, 1)\n",
    "            #batch_seq_steering_angles[i] = seq_steering_angles\n",
    "\n",
    "        # for ru\n",
    "        yield batch_seq_images, batch_seq_steering_angles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "test_gen = data_generator(test_set, seq_len)\n",
    "batch_seq_images, batch_seq_steering_angles = next(test_gen)\n",
    "\n",
    "waste = rcnn_model.predict(batch_seq_images, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving Model...\n",
      "Model Saved.\n",
      "WARNING:tensorflow:From /home/basic/anaconda/envs/udacity/lib/python3.5/site-packages/keras/callbacks.py:588 in set_model.: histogram_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.histogram. Note that tf.summary.histogram uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on their scope.\n",
      "WARNING:tensorflow:From /home/basic/anaconda/envs/udacity/lib/python3.5/site-packages/keras/callbacks.py:605 in set_model.: image_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.image. Note that tf.summary.histogram uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, the max_images argument was renamed to max_outputs.\n",
      "WARNING:tensorflow:From /home/basic/anaconda/envs/udacity/lib/python3.5/site-packages/keras/callbacks.py:588 in set_model.: histogram_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.histogram. Note that tf.summary.histogram uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on their scope.\n",
      "WARNING:tensorflow:From /home/basic/anaconda/envs/udacity/lib/python3.5/site-packages/keras/callbacks.py:605 in set_model.: image_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.image. Note that tf.summary.histogram uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, the max_images argument was renamed to max_outputs.\n",
      "WARNING:tensorflow:From /home/basic/anaconda/envs/udacity/lib/python3.5/site-packages/keras/callbacks.py:588 in set_model.: histogram_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.histogram. Note that tf.summary.histogram uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on their scope.\n",
      "WARNING:tensorflow:From /home/basic/anaconda/envs/udacity/lib/python3.5/site-packages/keras/callbacks.py:605 in set_model.: image_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.image. Note that tf.summary.histogram uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, the max_images argument was renamed to max_outputs.\n",
      "WARNING:tensorflow:From /home/basic/anaconda/envs/udacity/lib/python3.5/site-packages/keras/callbacks.py:588 in set_model.: histogram_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.histogram. Note that tf.summary.histogram uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on their scope.\n",
      "WARNING:tensorflow:From /home/basic/anaconda/envs/udacity/lib/python3.5/site-packages/keras/callbacks.py:605 in set_model.: image_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.image. Note that tf.summary.histogram uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, the max_images argument was renamed to max_outputs.\n",
      "WARNING:tensorflow:From /home/basic/anaconda/envs/udacity/lib/python3.5/site-packages/keras/callbacks.py:588 in set_model.: histogram_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.histogram. Note that tf.summary.histogram uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on their scope.\n",
      "WARNING:tensorflow:From /home/basic/anaconda/envs/udacity/lib/python3.5/site-packages/keras/callbacks.py:605 in set_model.: image_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.image. Note that tf.summary.histogram uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, the max_images argument was renamed to max_outputs.\n",
      "WARNING:tensorflow:From /home/basic/anaconda/envs/udacity/lib/python3.5/site-packages/keras/callbacks.py:588 in set_model.: histogram_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.histogram. Note that tf.summary.histogram uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on their scope.\n",
      "WARNING:tensorflow:From /home/basic/anaconda/envs/udacity/lib/python3.5/site-packages/keras/callbacks.py:605 in set_model.: image_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.image. Note that tf.summary.histogram uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, the max_images argument was renamed to max_outputs.\n",
      "WARNING:tensorflow:From /home/basic/anaconda/envs/udacity/lib/python3.5/site-packages/keras/callbacks.py:588 in set_model.: histogram_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.histogram. Note that tf.summary.histogram uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on their scope.\n",
      "WARNING:tensorflow:From /home/basic/anaconda/envs/udacity/lib/python3.5/site-packages/keras/callbacks.py:605 in set_model.: image_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.image. Note that tf.summary.histogram uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, the max_images argument was renamed to max_outputs.\n",
      "WARNING:tensorflow:From /home/basic/anaconda/envs/udacity/lib/python3.5/site-packages/keras/callbacks.py:588 in set_model.: histogram_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.histogram. Note that tf.summary.histogram uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on their scope.\n",
      "WARNING:tensorflow:From /home/basic/anaconda/envs/udacity/lib/python3.5/site-packages/keras/callbacks.py:605 in set_model.: image_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.image. Note that tf.summary.histogram uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, the max_images argument was renamed to max_outputs.\n",
      "WARNING:tensorflow:From /home/basic/anaconda/envs/udacity/lib/python3.5/site-packages/keras/callbacks.py:588 in set_model.: histogram_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.histogram. Note that tf.summary.histogram uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on their scope.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/basic/anaconda/envs/udacity/lib/python3.5/site-packages/keras/callbacks.py:605 in set_model.: image_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.image. Note that tf.summary.histogram uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, the max_images argument was renamed to max_outputs.\n",
      "WARNING:tensorflow:From /home/basic/anaconda/envs/udacity/lib/python3.5/site-packages/keras/callbacks.py:588 in set_model.: histogram_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.histogram. Note that tf.summary.histogram uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on their scope.\n",
      "WARNING:tensorflow:From /home/basic/anaconda/envs/udacity/lib/python3.5/site-packages/keras/callbacks.py:605 in set_model.: image_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.image. Note that tf.summary.histogram uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, the max_images argument was renamed to max_outputs.\n",
      "WARNING:tensorflow:From /home/basic/anaconda/envs/udacity/lib/python3.5/site-packages/keras/callbacks.py:588 in set_model.: histogram_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.histogram. Note that tf.summary.histogram uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on their scope.\n",
      "WARNING:tensorflow:From /home/basic/anaconda/envs/udacity/lib/python3.5/site-packages/keras/callbacks.py:605 in set_model.: image_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.image. Note that tf.summary.histogram uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, the max_images argument was renamed to max_outputs.\n",
      "WARNING:tensorflow:From /home/basic/anaconda/envs/udacity/lib/python3.5/site-packages/keras/callbacks.py:588 in set_model.: histogram_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.histogram. Note that tf.summary.histogram uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on their scope.\n",
      "WARNING:tensorflow:From /home/basic/anaconda/envs/udacity/lib/python3.5/site-packages/keras/callbacks.py:605 in set_model.: image_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.image. Note that tf.summary.histogram uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, the max_images argument was renamed to max_outputs.\n",
      "WARNING:tensorflow:From /home/basic/anaconda/envs/udacity/lib/python3.5/site-packages/keras/callbacks.py:612 in set_model.: histogram_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.histogram. Note that tf.summary.histogram uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on their scope.\n",
      "WARNING:tensorflow:From /home/basic/anaconda/envs/udacity/lib/python3.5/site-packages/keras/callbacks.py:588 in set_model.: histogram_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.histogram. Note that tf.summary.histogram uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on their scope.\n",
      "WARNING:tensorflow:From /home/basic/anaconda/envs/udacity/lib/python3.5/site-packages/keras/callbacks.py:605 in set_model.: image_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.image. Note that tf.summary.histogram uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, the max_images argument was renamed to max_outputs.\n",
      "WARNING:tensorflow:From /home/basic/anaconda/envs/udacity/lib/python3.5/site-packages/keras/callbacks.py:588 in set_model.: histogram_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.histogram. Note that tf.summary.histogram uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on their scope.\n",
      "WARNING:tensorflow:From /home/basic/anaconda/envs/udacity/lib/python3.5/site-packages/keras/callbacks.py:605 in set_model.: image_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.image. Note that tf.summary.histogram uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, the max_images argument was renamed to max_outputs.\n",
      "WARNING:tensorflow:From /home/basic/anaconda/envs/udacity/lib/python3.5/site-packages/keras/callbacks.py:588 in set_model.: histogram_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.histogram. Note that tf.summary.histogram uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on their scope.\n",
      "WARNING:tensorflow:From /home/basic/anaconda/envs/udacity/lib/python3.5/site-packages/keras/callbacks.py:605 in set_model.: image_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.image. Note that tf.summary.histogram uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, the max_images argument was renamed to max_outputs.\n",
      "WARNING:tensorflow:From /home/basic/anaconda/envs/udacity/lib/python3.5/site-packages/keras/callbacks.py:588 in set_model.: histogram_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.histogram. Note that tf.summary.histogram uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on their scope.\n",
      "WARNING:tensorflow:From /home/basic/anaconda/envs/udacity/lib/python3.5/site-packages/keras/callbacks.py:605 in set_model.: image_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.image. Note that tf.summary.histogram uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, the max_images argument was renamed to max_outputs.\n",
      "WARNING:tensorflow:From /home/basic/anaconda/envs/udacity/lib/python3.5/site-packages/keras/callbacks.py:588 in set_model.: histogram_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.histogram. Note that tf.summary.histogram uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on their scope.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/basic/anaconda/envs/udacity/lib/python3.5/site-packages/keras/callbacks.py:605 in set_model.: image_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.image. Note that tf.summary.histogram uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, the max_images argument was renamed to max_outputs.\n",
      "WARNING:tensorflow:From /home/basic/anaconda/envs/udacity/lib/python3.5/site-packages/keras/callbacks.py:588 in set_model.: histogram_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.histogram. Note that tf.summary.histogram uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on their scope.\n",
      "WARNING:tensorflow:From /home/basic/anaconda/envs/udacity/lib/python3.5/site-packages/keras/callbacks.py:605 in set_model.: image_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.image. Note that tf.summary.histogram uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, the max_images argument was renamed to max_outputs.\n",
      "WARNING:tensorflow:From /home/basic/anaconda/envs/udacity/lib/python3.5/site-packages/keras/callbacks.py:588 in set_model.: histogram_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.histogram. Note that tf.summary.histogram uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on their scope.\n",
      "WARNING:tensorflow:From /home/basic/anaconda/envs/udacity/lib/python3.5/site-packages/keras/callbacks.py:605 in set_model.: image_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.image. Note that tf.summary.histogram uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, the max_images argument was renamed to max_outputs.\n",
      "WARNING:tensorflow:From /home/basic/anaconda/envs/udacity/lib/python3.5/site-packages/keras/callbacks.py:588 in set_model.: histogram_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.histogram. Note that tf.summary.histogram uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on their scope.\n",
      "WARNING:tensorflow:From /home/basic/anaconda/envs/udacity/lib/python3.5/site-packages/keras/callbacks.py:605 in set_model.: image_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.image. Note that tf.summary.histogram uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, the max_images argument was renamed to max_outputs.\n",
      "WARNING:tensorflow:From /home/basic/anaconda/envs/udacity/lib/python3.5/site-packages/keras/callbacks.py:588 in set_model.: histogram_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.histogram. Note that tf.summary.histogram uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on their scope.\n",
      "WARNING:tensorflow:From /home/basic/anaconda/envs/udacity/lib/python3.5/site-packages/keras/callbacks.py:605 in set_model.: image_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.image. Note that tf.summary.histogram uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, the max_images argument was renamed to max_outputs.\n",
      "WARNING:tensorflow:From /home/basic/anaconda/envs/udacity/lib/python3.5/site-packages/keras/callbacks.py:588 in set_model.: histogram_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.histogram. Note that tf.summary.histogram uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on their scope.\n",
      "WARNING:tensorflow:From /home/basic/anaconda/envs/udacity/lib/python3.5/site-packages/keras/callbacks.py:605 in set_model.: image_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.image. Note that tf.summary.histogram uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, the max_images argument was renamed to max_outputs.\n",
      "WARNING:tensorflow:From /home/basic/anaconda/envs/udacity/lib/python3.5/site-packages/keras/callbacks.py:588 in set_model.: histogram_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.histogram. Note that tf.summary.histogram uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on their scope.\n",
      "WARNING:tensorflow:From /home/basic/anaconda/envs/udacity/lib/python3.5/site-packages/keras/callbacks.py:605 in set_model.: image_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.image. Note that tf.summary.histogram uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, the max_images argument was renamed to max_outputs.\n",
      "WARNING:tensorflow:From /home/basic/anaconda/envs/udacity/lib/python3.5/site-packages/keras/callbacks.py:618 in set_model.: merge_all_summaries (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.merge_all.\n",
      "WARNING:tensorflow:From /home/basic/anaconda/envs/udacity/lib/python3.5/site-packages/tensorflow/python/ops/logging_ops.py:264 in merge_all_summaries.: merge_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.merge.\n",
      "Beginning training\n",
      "Epoch 1/250\n",
      "1320/1320 [==============================] - 69s - loss: 0.0230 - val_loss: 0.0222\n",
      "Epoch 2/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0205 - val_loss: 0.0188\n",
      "Epoch 3/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0187 - val_loss: 0.0177\n",
      "Epoch 4/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0174 - val_loss: 0.0175\n",
      "Epoch 5/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0167 - val_loss: 0.0156\n",
      "Epoch 6/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0166 - val_loss: 0.0159\n",
      "Epoch 7/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0153 - val_loss: 0.0149\n",
      "Epoch 8/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0151 - val_loss: 0.0149\n",
      "Epoch 9/250\n",
      "1320/1320 [==============================] - 61s - loss: 0.0150 - val_loss: 0.0148\n",
      "Epoch 10/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0146 - val_loss: 0.0151\n",
      "Epoch 11/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0148 - val_loss: 0.0146\n",
      "Epoch 12/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0146 - val_loss: 0.0150\n",
      "Epoch 13/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0138 - val_loss: 0.0140\n",
      "Epoch 14/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0136 - val_loss: 0.0139\n",
      "Epoch 15/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0138 - val_loss: 0.0141\n",
      "Epoch 16/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0139 - val_loss: 0.0140\n",
      "Epoch 17/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0136 - val_loss: 0.0140\n",
      "Epoch 18/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0131 - val_loss: 0.0133\n",
      "Epoch 19/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0134 - val_loss: 0.0135\n",
      "Epoch 20/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0132 - val_loss: 0.0137\n",
      "Epoch 21/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0129 - val_loss: 0.0136\n",
      "Epoch 22/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0130 - val_loss: 0.0144\n",
      "Epoch 23/250\n",
      "1320/1320 [==============================] - 61s - loss: 0.0130 - val_loss: 0.0136\n",
      "Epoch 24/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0130 - val_loss: 0.0136\n",
      "Epoch 25/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0129 - val_loss: 0.0131\n",
      "Epoch 26/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0126 - val_loss: 0.0133\n",
      "Epoch 27/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0129 - val_loss: 0.0133\n",
      "Epoch 28/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0128 - val_loss: 0.0134\n",
      "Epoch 29/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0126 - val_loss: 0.0131\n",
      "Epoch 30/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0128 - val_loss: 0.0133\n",
      "Epoch 31/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0126 - val_loss: 0.0139\n",
      "Epoch 32/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0129 - val_loss: 0.0132\n",
      "Epoch 33/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0123 - val_loss: 0.0131\n",
      "Epoch 34/250\n",
      "1320/1320 [==============================] - 61s - loss: 0.0127 - val_loss: 0.0132\n",
      "Epoch 35/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0124 - val_loss: 0.0132\n",
      "Epoch 36/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0123 - val_loss: 0.0133\n",
      "Epoch 37/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0127 - val_loss: 0.0136\n",
      "Epoch 38/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0130 - val_loss: 0.0132\n",
      "Epoch 39/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0125 - val_loss: 0.0132\n",
      "Epoch 40/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0126 - val_loss: 0.0130\n",
      "Epoch 41/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0121 - val_loss: 0.0132\n",
      "Epoch 42/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0122 - val_loss: 0.0130\n",
      "Epoch 43/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0121 - val_loss: 0.0134\n",
      "Epoch 44/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0124 - val_loss: 0.0129\n",
      "Epoch 45/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0122 - val_loss: 0.0124\n",
      "Epoch 46/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0125 - val_loss: 0.0131\n",
      "Epoch 47/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0120 - val_loss: 0.0130\n",
      "Epoch 48/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0121 - val_loss: 0.0131\n",
      "Epoch 49/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0119 - val_loss: 0.0128\n",
      "Epoch 50/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0121 - val_loss: 0.0133\n",
      "Epoch 51/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0125 - val_loss: 0.0128\n",
      "Epoch 52/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0122 - val_loss: 0.0125\n",
      "Epoch 53/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0122 - val_loss: 0.0128\n",
      "Epoch 54/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0114 - val_loss: 0.0127\n",
      "Epoch 55/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0120 - val_loss: 0.0125\n",
      "Epoch 56/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0118 - val_loss: 0.0126\n",
      "Epoch 57/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0121 - val_loss: 0.0129\n",
      "Epoch 58/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0116 - val_loss: 0.0128\n",
      "Epoch 59/250\n",
      "1320/1320 [==============================] - 59s - loss: 0.0121 - val_loss: 0.0128\n",
      "Epoch 60/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0118 - val_loss: 0.0130\n",
      "Epoch 61/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0118 - val_loss: 0.0126\n",
      "Epoch 62/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0117 - val_loss: 0.0132\n",
      "Epoch 63/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0120 - val_loss: 0.0122\n",
      "Epoch 64/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0115 - val_loss: 0.0125\n",
      "Epoch 65/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0114 - val_loss: 0.0128\n",
      "Epoch 66/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0115 - val_loss: 0.0128\n",
      "Epoch 67/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0119 - val_loss: 0.0129\n",
      "Epoch 68/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0116 - val_loss: 0.0127\n",
      "Epoch 69/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0112 - val_loss: 0.0124\n",
      "Epoch 70/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0117 - val_loss: 0.0127\n",
      "Epoch 71/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0117 - val_loss: 0.0128\n",
      "Epoch 72/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0118 - val_loss: 0.0125\n",
      "Epoch 73/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0114 - val_loss: 0.0121\n",
      "Epoch 74/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0112 - val_loss: 0.0122\n",
      "Epoch 75/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0112 - val_loss: 0.0130\n",
      "Epoch 76/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0115 - val_loss: 0.0128\n",
      "Epoch 77/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0116 - val_loss: 0.0126\n",
      "Epoch 78/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0114 - val_loss: 0.0127\n",
      "Epoch 79/250\n",
      "1320/1320 [==============================] - 61s - loss: 0.0116 - val_loss: 0.0129\n",
      "Epoch 80/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0114 - val_loss: 0.0128\n",
      "Epoch 81/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0114 - val_loss: 0.0128\n",
      "Epoch 82/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0112 - val_loss: 0.0124\n",
      "Epoch 83/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0110 - val_loss: 0.0128\n",
      "Epoch 84/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0113 - val_loss: 0.0125\n",
      "Epoch 85/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0115 - val_loss: 0.0127\n",
      "Epoch 86/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0110 - val_loss: 0.0132\n",
      "Epoch 87/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0113 - val_loss: 0.0128\n",
      "Epoch 88/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0111 - val_loss: 0.0129\n",
      "Epoch 89/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0113 - val_loss: 0.0128\n",
      "Epoch 90/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0110 - val_loss: 0.0125\n",
      "Epoch 91/250\n",
      "1320/1320 [==============================] - 61s - loss: 0.0110 - val_loss: 0.0126\n",
      "Epoch 92/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0114 - val_loss: 0.0132\n",
      "Epoch 93/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0112 - val_loss: 0.0131\n",
      "Epoch 94/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0112 - val_loss: 0.0128\n",
      "Epoch 95/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0116 - val_loss: 0.0130\n",
      "Epoch 96/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1320/1320 [==============================] - 60s - loss: 0.0108 - val_loss: 0.0126\n",
      "Epoch 97/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0113 - val_loss: 0.0129\n",
      "Epoch 98/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0114 - val_loss: 0.0122\n",
      "Epoch 99/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0112 - val_loss: 0.0125\n",
      "Epoch 100/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0109 - val_loss: 0.0125\n",
      "Epoch 101/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0112 - val_loss: 0.0122\n",
      "Epoch 102/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0110 - val_loss: 0.0123\n",
      "Epoch 103/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0111 - val_loss: 0.0127\n",
      "Epoch 104/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0106 - val_loss: 0.0127\n",
      "Epoch 105/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0111 - val_loss: 0.0125\n",
      "Epoch 106/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0108 - val_loss: 0.0129\n",
      "Epoch 107/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0114 - val_loss: 0.0126\n",
      "Epoch 108/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0111 - val_loss: 0.0123\n",
      "Epoch 109/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0107 - val_loss: 0.0123\n",
      "Epoch 110/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0110 - val_loss: 0.0127\n",
      "Epoch 111/250\n",
      "1320/1320 [==============================] - 59s - loss: 0.0106 - val_loss: 0.0125\n",
      "Epoch 112/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0113 - val_loss: 0.0126\n",
      "Epoch 113/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0108 - val_loss: 0.0125\n",
      "Epoch 114/250\n",
      "1320/1320 [==============================] - 61s - loss: 0.0107 - val_loss: 0.0132\n",
      "Epoch 115/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0106 - val_loss: 0.0126\n",
      "Epoch 116/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0112 - val_loss: 0.0126\n",
      "Epoch 117/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0109 - val_loss: 0.0121\n",
      "Epoch 118/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0110 - val_loss: 0.0123\n",
      "Epoch 119/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0108 - val_loss: 0.0128\n",
      "Epoch 120/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0108 - val_loss: 0.0123\n",
      "Epoch 121/250\n",
      "1320/1320 [==============================] - 61s - loss: 0.0110 - val_loss: 0.0123\n",
      "Epoch 122/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0109 - val_loss: 0.0130\n",
      "Epoch 123/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0108 - val_loss: 0.0125\n",
      "Epoch 124/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0106 - val_loss: 0.0123\n",
      "Epoch 125/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0105 - val_loss: 0.0127\n",
      "Epoch 126/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0105 - val_loss: 0.0124\n",
      "Epoch 127/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0110 - val_loss: 0.0128\n",
      "Epoch 128/250\n",
      "1320/1320 [==============================] - 61s - loss: 0.0103 - val_loss: 0.0122\n",
      "Epoch 129/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0104 - val_loss: 0.0126\n",
      "Epoch 130/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0107 - val_loss: 0.0129\n",
      "Epoch 131/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0104 - val_loss: 0.0123\n",
      "Epoch 132/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0109 - val_loss: 0.0123\n",
      "Epoch 133/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0105 - val_loss: 0.0125\n",
      "Epoch 134/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0105 - val_loss: 0.0127\n",
      "Epoch 135/250\n",
      "1320/1320 [==============================] - 61s - loss: 0.0106 - val_loss: 0.0126\n",
      "Epoch 136/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0104 - val_loss: 0.0123\n",
      "Epoch 137/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0104 - val_loss: 0.0123\n",
      "Epoch 138/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0104 - val_loss: 0.0129\n",
      "Epoch 139/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0104 - val_loss: 0.0126\n",
      "Epoch 140/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0107 - val_loss: 0.0123\n",
      "Epoch 141/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0105 - val_loss: 0.0123\n",
      "Epoch 142/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0105 - val_loss: 0.0125\n",
      "Epoch 143/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0107 - val_loss: 0.0125\n",
      "Epoch 144/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0102 - val_loss: 0.0126\n",
      "Epoch 145/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0103 - val_loss: 0.0128\n",
      "Epoch 146/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0103 - val_loss: 0.0122\n",
      "Epoch 147/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0102 - val_loss: 0.0125\n",
      "Epoch 148/250\n",
      "1320/1320 [==============================] - 61s - loss: 0.0106 - val_loss: 0.0130\n",
      "Epoch 149/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0104 - val_loss: 0.0126\n",
      "Epoch 150/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0103 - val_loss: 0.0126\n",
      "Epoch 151/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0107 - val_loss: 0.0129\n",
      "Epoch 152/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0100 - val_loss: 0.0127\n",
      "Epoch 153/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0104 - val_loss: 0.0124\n",
      "Epoch 154/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0106 - val_loss: 0.0126\n",
      "Epoch 155/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0106 - val_loss: 0.0124\n",
      "Epoch 156/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0097 - val_loss: 0.0127\n",
      "Epoch 157/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0101 - val_loss: 0.0128\n",
      "Epoch 158/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0101 - val_loss: 0.0126\n",
      "Epoch 159/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0103 - val_loss: 0.0124\n",
      "Epoch 160/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0104 - val_loss: 0.0130\n",
      "Epoch 161/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0102 - val_loss: 0.0123\n",
      "Epoch 162/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0100 - val_loss: 0.0127\n",
      "Epoch 163/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0104 - val_loss: 0.0126\n",
      "Epoch 164/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0100 - val_loss: 0.0125\n",
      "Epoch 165/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0099 - val_loss: 0.0129\n",
      "Epoch 166/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0104 - val_loss: 0.0127\n",
      "Epoch 167/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0106 - val_loss: 0.0128\n",
      "Epoch 168/250\n",
      "1320/1320 [==============================] - 59s - loss: 0.0106 - val_loss: 0.0123\n",
      "Epoch 169/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0104 - val_loss: 0.0127\n",
      "Epoch 170/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0102 - val_loss: 0.0122\n",
      "Epoch 171/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0099 - val_loss: 0.0131\n",
      "Epoch 172/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0105 - val_loss: 0.0128\n",
      "Epoch 173/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0100 - val_loss: 0.0127\n",
      "Epoch 174/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0102 - val_loss: 0.0128\n",
      "Epoch 175/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0099 - val_loss: 0.0121\n",
      "Epoch 176/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0099 - val_loss: 0.0128\n",
      "Epoch 177/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0099 - val_loss: 0.0124\n",
      "Epoch 178/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0099 - val_loss: 0.0129\n",
      "Epoch 179/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0102 - val_loss: 0.0129\n",
      "Epoch 180/250\n",
      "1320/1320 [==============================] - 61s - loss: 0.0102 - val_loss: 0.0127\n",
      "Epoch 181/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0103 - val_loss: 0.0125\n",
      "Epoch 182/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0104 - val_loss: 0.0127\n",
      "Epoch 183/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0102 - val_loss: 0.0127\n",
      "Epoch 184/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0101 - val_loss: 0.0128\n",
      "Epoch 185/250\n",
      "1320/1320 [==============================] - 59s - loss: 0.0107 - val_loss: 0.0125\n",
      "Epoch 186/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0099 - val_loss: 0.0129\n",
      "Epoch 187/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0101 - val_loss: 0.0126\n",
      "Epoch 188/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0101 - val_loss: 0.0129\n",
      "Epoch 189/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0100 - val_loss: 0.0128\n",
      "Epoch 190/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0102 - val_loss: 0.0122\n",
      "Epoch 191/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0099 - val_loss: 0.0125\n",
      "Epoch 192/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0101 - val_loss: 0.0123\n",
      "Epoch 193/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0099 - val_loss: 0.0128\n",
      "Epoch 194/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0100 - val_loss: 0.0128\n",
      "Epoch 195/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0100 - val_loss: 0.0130\n",
      "Epoch 196/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0095 - val_loss: 0.0128\n",
      "Epoch 197/250\n",
      "1320/1320 [==============================] - 61s - loss: 0.0100 - val_loss: 0.0125\n",
      "Epoch 198/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0101 - val_loss: 0.0125\n",
      "Epoch 199/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0100 - val_loss: 0.0129\n",
      "Epoch 200/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0099 - val_loss: 0.0129\n",
      "Epoch 201/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0098 - val_loss: 0.0126\n",
      "Epoch 202/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0098 - val_loss: 0.0128\n",
      "Epoch 203/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0097 - val_loss: 0.0127\n",
      "Epoch 204/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0097 - val_loss: 0.0128\n",
      "Epoch 205/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0096 - val_loss: 0.0125\n",
      "Epoch 206/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0101 - val_loss: 0.0127\n",
      "Epoch 207/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0103 - val_loss: 0.0131\n",
      "Epoch 208/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0099 - val_loss: 0.0125\n",
      "Epoch 209/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0095 - val_loss: 0.0126\n",
      "Epoch 210/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0097 - val_loss: 0.0127\n",
      "Epoch 211/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0098 - val_loss: 0.0128\n",
      "Epoch 212/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0097 - val_loss: 0.0126\n",
      "Epoch 213/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0099 - val_loss: 0.0128\n",
      "Epoch 214/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0095 - val_loss: 0.0123\n",
      "Epoch 215/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0097 - val_loss: 0.0126\n",
      "Epoch 216/250\n",
      "1320/1320 [==============================] - 61s - loss: 0.0096 - val_loss: 0.0129\n",
      "Epoch 217/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0096 - val_loss: 0.0127\n",
      "Epoch 218/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0099 - val_loss: 0.0127\n",
      "Epoch 219/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0098 - val_loss: 0.0130\n",
      "Epoch 220/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0096 - val_loss: 0.0127\n",
      "Epoch 221/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0097 - val_loss: 0.0128\n",
      "Epoch 222/250\n",
      "1320/1320 [==============================] - 61s - loss: 0.0097 - val_loss: 0.0132\n",
      "Epoch 223/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0095 - val_loss: 0.0130\n",
      "Epoch 224/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0097 - val_loss: 0.0127\n",
      "Epoch 225/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0096 - val_loss: 0.0127\n",
      "Epoch 226/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0095 - val_loss: 0.0128\n",
      "Epoch 227/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0094 - val_loss: 0.0130\n",
      "Epoch 228/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0094 - val_loss: 0.0128\n",
      "Epoch 229/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0094 - val_loss: 0.0127\n",
      "Epoch 230/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0097 - val_loss: 0.0128\n",
      "Epoch 231/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0097 - val_loss: 0.0125\n",
      "Epoch 232/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0098 - val_loss: 0.0126\n",
      "Epoch 233/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0099 - val_loss: 0.0128\n",
      "Epoch 234/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0098 - val_loss: 0.0127\n",
      "Epoch 235/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0097 - val_loss: 0.0126\n",
      "Epoch 236/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0098 - val_loss: 0.0131\n",
      "Epoch 237/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0094 - val_loss: 0.0132\n",
      "Epoch 238/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0096 - val_loss: 0.0132\n",
      "Epoch 239/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0097 - val_loss: 0.0123\n",
      "Epoch 240/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0097 - val_loss: 0.0126\n",
      "Epoch 241/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0097 - val_loss: 0.0123\n",
      "Epoch 242/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0094 - val_loss: 0.0131\n",
      "Epoch 243/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0098 - val_loss: 0.0130\n",
      "Epoch 244/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0096 - val_loss: 0.0127\n",
      "Epoch 245/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0097 - val_loss: 0.0125\n",
      "Epoch 246/250\n",
      "1320/1320 [==============================] - 61s - loss: 0.0098 - val_loss: 0.0124\n",
      "Epoch 247/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0096 - val_loss: 0.0130\n",
      "Epoch 248/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0096 - val_loss: 0.0130\n",
      "Epoch 249/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0094 - val_loss: 0.0128\n",
      "Epoch 250/250\n",
      "1320/1320 [==============================] - 60s - loss: 0.0096 - val_loss: 0.0127\n",
      "Ending Training\n",
      "\n",
      "Training the model ended.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.callbacks import Callback\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import math\n",
    "\n",
    "class LifeCycleCallBack(keras.callbacks.Callback):\n",
    "    \n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        pass\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        pass\n",
    "\n",
    "    def on_batch_begin(self, batch, logs={}):\n",
    "        pass\n",
    "    \n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        \n",
    "    def on_train_begin(self, logs={}):\n",
    "        print('Beginning training')\n",
    "        self.losses = []\n",
    "        \n",
    "    def on_train_end(self, logs={}):\n",
    "        print('Ending Training')\n",
    "\n",
    "\n",
    "NUM_EPOCHS = 250\n",
    "        \n",
    "lifecycle_callback = LifeCycleCallBack()\n",
    "\n",
    "tensorboard = keras.callbacks.TensorBoard(log_dir='../../data/logs/steer_cnn_rnn/', histogram_freq=1, \n",
    "                                          write_graph=True, write_images=True)\n",
    "\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(\"../../data/weights/steer_cnn_rnn/model.h5\", monitor='val_loss', verbose=0, \n",
    "                                             save_best_only=True, save_weights_only=True, mode='auto', period=1)\n",
    "\n",
    "train_generator = data_generator(train_set, seq_len)\n",
    "valid_generator = data_generator(test_set, seq_len)\n",
    "\n",
    "samples_per_epoch = math.ceil((len(train_set) - seq_len)/(seq_len*BATCH_SIZE))*BATCH_SIZE\n",
    "nb_val_samples = math.ceil((len(test_set) - seq_len)/(seq_len*BATCH_SIZE))*5*BATCH_SIZE\n",
    "\n",
    "\n",
    "print(\"\\nSaving Model...\")\n",
    "\n",
    "model_json = rcnn_model.to_json()\n",
    "with open(\"../../data/weights/steer_cnn_rnn/model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "#model.save_weights(\"model.h5\")\n",
    "print(\"Model Saved.\")\n",
    "\n",
    "\n",
    "history = rcnn_model.fit_generator(train_generator,\n",
    "                              validation_data=valid_generator,\n",
    "                              samples_per_epoch=samples_per_epoch,\n",
    "                              nb_val_samples=nb_val_samples,\n",
    "                              nb_epoch=NUM_EPOCHS, verbose=1, \n",
    "                              callbacks=[lifecycle_callback, tensorboard, checkpoint])\n",
    "\n",
    "print(\"\\nTraining the model ended.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'val_loss'])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEWCAYAAABMoxE0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fec18363630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot train and valid loss curve\n",
    "\n",
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
